{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6KTQ0KNeTh2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Research**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rFq1FboeaO6"
      },
      "source": [
        "**Title:** Examining Input Noise Injection During Dropout in AlexNet for Robust CIFAR Dataset Image Recognition\n",
        "\n",
        "**Objective:** This research aims to investigate the impact of input noise injection on dropped neurons during the training phase of AlexNet, which is a pioneering convolutional neural network (CNN) for image classification tasks. We will use the CIFAR-10 and CIFAR-100 datasets to determine how input noise affects AlexNet's resilience to adversarial attacks and its ability to accurately classify images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCqSaVekfPP-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "### **Methodology:**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsB8wNZZfGaw"
      },
      "source": [
        "**Model Framework:** We will be working with AlexNet, a cornerstone in the development of CNNs for categorizing images.\n",
        "\n",
        "**Datasets for Testing:** The CIFAR-10 and CIFAR-100 datasets, encompassing a broad spectrum of image classes, will serve as our test bed to rigorously evaluate the model's robustness and consistency.\n",
        "\n",
        "**Noise Injection Strategy:** Our approach involves injecting stochastic noise directly to the input data corresponding to neurons that are dropped during the training process. This will help us assess how the network copes with the added uncertainty and whether this can enhance its robustness.\n",
        "\n",
        "**Training and Evaluation:** We will meticulously train AlexNet from the ground up, meticulously tuning the noise injection parameters and observing how these adjustments influence the network's learning efficacy.\n",
        "\n",
        "**Performance Metrics:** The success of our noise injection will be measured by contrasting the network's performance on standard testing images against its performance when challenged with adversarially modified images.\n",
        "\n",
        "**Update on the testing strategy:** We First determined the standard deviation of the dataset, which was 0.1595, and used this value as the base rate for applying noise. Noise levels were introduced at 5% (0.007975), 10% (0.01595), 30% (0.04785), and 50% (0.07975) relative to this base rate. For dropout probabilities, we applied the same series of noise levels across three different settings: at a 20% dropout probability, we employed noise rates of 5%, 10%, 30%, 50%, and 100% of the base rate; this pattern was consistently used for dropout probabilities of 25% and 30% as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl86pCELfzVn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Anticipated Challenges:**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NPYnSxwf5Zp"
      },
      "source": [
        "**Stabilizing the Model:** Injecting noise at the input level may introduce new complexities in the network’s learning process, necessitating careful recalibration of the training hyperparameters.\n",
        "\n",
        "**Noise Optimization:** We aim to pinpoint the precise noise level that strikes a balance between fortifying robustness and maintaining high accuracy on standard image sets.\n",
        "\n",
        "**Computational Demands:** The extensive computational resources required for training and evaluating numerous variations of the model under different noise conditions represent a significant logistical challenge.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6WvSpYtgyOg"
      },
      "source": [
        "### **Expected Outcomes:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa8RL-BRg2gR"
      },
      "source": [
        "**Robustness Advancements:** We anticipate our method will significantly boost AlexNet's defenses against deceptive inputs, leading to more reliable real-world application performance.\n",
        "\n",
        "**Insightful Discoveries:** This exploration is poised to shed light on the dynamics of noise in neural network training and its potential to create more resilient network architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCQe_hn7hAz3"
      },
      "source": [
        "### **Contribution to the Field:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bhGPgQyhH3P"
      },
      "source": [
        "With this project, we aim to enrich the neural network community with insights and methodologies that enhance the security of CNNs like AlexNet. Our findings could pave the way for new, more robust training techniques that harden neural networks against sophisticated adversarial tactics, especially in fields where security and reliability are paramount."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Yhw5kEQTu-7"
      },
      "source": [
        "## **Setting Up the Environment**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXFr06rdeRya",
        "outputId": "f8f18e66-e5d3-4045-a41a-3750c2f999b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio #installing pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiP7NHNyzWVN"
      },
      "source": [
        "###**Standard Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKuibtGmW8Fd"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import CIFAR10, MNIST\n",
        "from torchvision.transforms.v2 import ToTensor\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm1l-ye3XFu_",
        "outputId": "1182441a-34ea-4fb4-c7a3-fae99826efa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 62801218.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-10 transform - three channels, normalize with 3 means and 3 SDs\n",
        "cifar_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# CIFAR10 data\n",
        "train_dataset = CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "\n",
        "val_dataset = CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtBmAfetXWzO",
        "outputId": "1a5f780f-b2ef-406a-cc10-87ed8d1c0bfa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# dataloaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lAi8svBcyHG",
        "outputId": "22956ce0-1006-41b0-88f0-accb31c49633"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed standard deviation of the dataset: tensor(0.1529)\n"
          ]
        }
      ],
      "source": [
        "#Compute Standard Deviation of the Dataset\n",
        "\n",
        "def compute_dataset_std(train_loader):\n",
        "    # We square sum the standard deviations of each batch to compute the total variance, then take the square root.\n",
        "    var_sum = 0\n",
        "    count = 0\n",
        "    for data, _ in train_loader:\n",
        "        var_sum += data.var([0, 2, 3], unbiased=False).sum()\n",
        "        count += data.size(0)\n",
        "    total_variance = var_sum / count\n",
        "    dataset_std = torch.sqrt(total_variance)\n",
        "    return dataset_std.mean()\n",
        "\n",
        "dataset_std = compute_dataset_std(train_loader)\n",
        "print(\"Computed standard deviation of the dataset:\", dataset_std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1YVX7oKX21p"
      },
      "outputs": [],
      "source": [
        "# Loss fuction and optimizer\n",
        "def get_crit_and_opt(net):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    return criterion, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_C2NQq6z-T-"
      },
      "source": [
        "CIFAR10net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQiymaj5z6bv"
      },
      "outputs": [],
      "source": [
        "class CIFAR10Net(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.25):\n",
        "        super(CIFAR10Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        dummy_input = torch.autograd.Variable(torch.zeros(1, 3, 32, 32))\n",
        "        dummy_output = self.pool(self.conv4(self.pool(self.conv3(self.pool(self.conv2(self.pool(self.conv1(dummy_input))))))))\n",
        "        self.final_feature_map_size = dummy_output.size(-1) * dummy_output.size(-2) * dummy_output.size(-3)\n",
        "        self.fc1 = nn.Linear(self.final_feature_map_size, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.pool(self.conv1(x)))\n",
        "        x = F.relu(self.pool(self.conv2(x)))\n",
        "        x = F.relu(self.pool(self.conv3(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.pool(self.conv4(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = x.view(-1, self.final_feature_map_size)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25wGj5IbYXNX"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "\n",
        "    \"\"\"Computes and stores an average and current value.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esOsPBekYams"
      },
      "outputs": [],
      "source": [
        "def error_rate(output, target, topk=(1,)):\n",
        "\n",
        "    \"\"\"Computes the top-k error rate for the specified values of k.\"\"\"\n",
        "\n",
        "    maxk = max(topk) # largest k we'll need to work with\n",
        "    batch_size = target.size(0) # determine batch size\n",
        "\n",
        "    # get maxk best predictions for each item in the batch, both values and indices\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "\n",
        "    # reshape predictions and targets and compare them element-wise\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk: # for each top-k accuracy we want\n",
        "\n",
        "        # num correct\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        # num incorrect\n",
        "        wrong_k = batch_size - correct_k\n",
        "        # as a percentage\n",
        "        res.append(wrong_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce0QrI-OYgOT"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ7dv6Nmc17T"
      },
      "outputs": [],
      "source": [
        "# training function - 1 epoch\n",
        "def train(\n",
        "    train_loader,\n",
        "    model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    epoch,\n",
        "    epochs,\n",
        "    print_freq = 100,\n",
        "    verbose = True\n",
        "):\n",
        "\n",
        "    # track average and worst losses\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # set training mode\n",
        "    model.train()\n",
        "\n",
        "    # iterate over data - automatically shuffled\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        # put batch of image tensors on GPU\n",
        "        images = images.to(device)\n",
        "        # put batch of label tensors on GPU\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # model output\n",
        "        outputs = model(images)\n",
        "\n",
        "        # loss computation\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # back propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # update meter with the value of the loss once for each item in the batch\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "\n",
        "        # logging during epoch\n",
        "        if i % print_freq == 0 and verbose == True:\n",
        "            print(\n",
        "                f'Epoch: [{epoch+1}/{epochs}][{i:4}/{len(train_loader)}]\\t'\n",
        "                f'Loss: {losses.val:.4f} ({losses.avg:.4f} on avg)'\n",
        "            )\n",
        "\n",
        "    # log again at end of epoch\n",
        "    print(f'\\n* Epoch: [{epoch+1}/{epochs}]\\tTrain loss: {losses.avg:.3f}\\n')\n",
        "\n",
        "    return losses.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-6yzrF8c8ON"
      },
      "outputs": [],
      "source": [
        "# val function\n",
        "def validate(\n",
        "    val_loader,\n",
        "    model,\n",
        "    criterion,\n",
        "    epoch,\n",
        "    epochs,\n",
        "    print_freq = 100,\n",
        "    verbose = True\n",
        "):\n",
        "\n",
        "    # track average and worst losses and batch-wise top-1 and top-5 accuracies\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # set evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # iterate over data - automatically shuffled\n",
        "    for i, (images, labels) in enumerate(val_loader):\n",
        "\n",
        "        # put batch of image tensors on GPU\n",
        "        images = images.to(device)\n",
        "        # put batch of label tensors on GPU\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # model output\n",
        "        output = model(images)\n",
        "\n",
        "        # loss computation\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        # top-1 and top-5 accuracy on this batch\n",
        "        err1, err5, = error_rate(output.data, labels, topk=(1, 5))\n",
        "\n",
        "        # update meters with the value of the loss once for each item in the batch\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        # update meters with top-1 and top-5 accuracy on this batch once for each item in the batch\n",
        "        top1.update(err1.item(), images.size(0))\n",
        "        top5.update(err5.item(), images.size(0))\n",
        "\n",
        "        # logging during epoch\n",
        "        if i % print_freq == 0 and verbose == True:\n",
        "            print(\n",
        "                f'Test (on val set): [{epoch+1}/{epochs}][{i:4}/{len(val_loader)}]\\t'\n",
        "                f'Loss: {losses.val:.4f} ({losses.avg:.4f} on avg)\\t'\n",
        "                f'Top-1 err: {top1.val:.4f} ({top1.avg:.4f} on avg)\\t'\n",
        "                f'Top-5 err: {top5.val:.4f} ({top5.avg:.4f} on avg)'\n",
        "            )\n",
        "\n",
        "    # logging for end of epoch\n",
        "    print(\n",
        "        f'\\n* Epoch: [{epoch+1}/{epochs}]\\t'\n",
        "        f'Test loss: {losses.avg:.3f}\\t'\n",
        "        f'Top-1 err: {top1.avg:.3f}\\t'\n",
        "        f'Top-5 err: {top5.avg:.3f}\\n'\n",
        "    )\n",
        "\n",
        "    # avergae top-1 and top-5 accuracies batch-wise, and average loss batch-wise\n",
        "    return top1.avg, top5.avg, losses.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J0foegAYwus"
      },
      "outputs": [],
      "source": [
        "# best error rates so far\n",
        "best_err1 = 100\n",
        "best_err5 = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q8ujmRDdMjn",
        "outputId": "d1b8296d-fc0f-4ca8-e2c7-88da075876ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1/10][   0/1563]\tLoss: 2.3070 (2.3070 on avg)\n",
            "Epoch: [1/10][ 100/1563]\tLoss: 2.3023 (2.3033 on avg)\n",
            "Epoch: [1/10][ 200/1563]\tLoss: 2.3026 (2.3031 on avg)\n",
            "Epoch: [1/10][ 300/1563]\tLoss: 2.3011 (2.3028 on avg)\n",
            "Epoch: [1/10][ 400/1563]\tLoss: 2.3039 (2.3026 on avg)\n",
            "Epoch: [1/10][ 500/1563]\tLoss: 2.2998 (2.3025 on avg)\n",
            "Epoch: [1/10][ 600/1563]\tLoss: 2.3012 (2.3022 on avg)\n",
            "Epoch: [1/10][ 700/1563]\tLoss: 2.3023 (2.3021 on avg)\n",
            "Epoch: [1/10][ 800/1563]\tLoss: 2.3032 (2.3019 on avg)\n",
            "Epoch: [1/10][ 900/1563]\tLoss: 2.2975 (2.3017 on avg)\n",
            "Epoch: [1/10][1000/1563]\tLoss: 2.3008 (2.3014 on avg)\n",
            "Epoch: [1/10][1100/1563]\tLoss: 2.3036 (2.3011 on avg)\n",
            "Epoch: [1/10][1200/1563]\tLoss: 2.2981 (2.3007 on avg)\n",
            "Epoch: [1/10][1300/1563]\tLoss: 2.2958 (2.3001 on avg)\n",
            "Epoch: [1/10][1400/1563]\tLoss: 2.2740 (2.2992 on avg)\n",
            "Epoch: [1/10][1500/1563]\tLoss: 2.2727 (2.2981 on avg)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "* Epoch: [1/10]\tTrain loss: 2.297\n",
            "\n",
            "Test (on val set): [1/10][   0/313]\tLoss: 2.2467 (2.2467 on avg)\tTop-1 err: 90.6250 (90.6250 on avg)\tTop-5 err: 31.2500 (31.2500 on avg)\n",
            "Test (on val set): [1/10][ 100/313]\tLoss: 2.2311 (2.2575 on avg)\tTop-1 err: 93.7500 (84.9010 on avg)\tTop-5 err: 21.8750 (36.8812 on avg)\n",
            "Test (on val set): [1/10][ 200/313]\tLoss: 2.2832 (2.2572 on avg)\tTop-1 err: 93.7500 (84.9658 on avg)\tTop-5 err: 50.0000 (36.3806 on avg)\n",
            "Test (on val set): [1/10][ 300/313]\tLoss: 2.2676 (2.2573 on avg)\tTop-1 err: 87.5000 (84.7799 on avg)\tTop-5 err: 34.3750 (36.5864 on avg)\n",
            "\n",
            "* Epoch: [1/10]\tTest loss: 2.257\tTop-1 err: 84.780\tTop-5 err: 36.560\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 25.52 1.93 \n",
            "\n",
            "Epoch: [2/10][   0/1563]\tLoss: 2.2773 (2.2773 on avg)\n",
            "Epoch: [2/10][ 100/1563]\tLoss: 2.2673 (2.2408 on avg)\n",
            "Epoch: [2/10][ 200/1563]\tLoss: 2.1685 (2.2040 on avg)\n",
            "Epoch: [2/10][ 300/1563]\tLoss: 1.8824 (2.1580 on avg)\n",
            "Epoch: [2/10][ 400/1563]\tLoss: 2.1473 (2.1299 on avg)\n",
            "Epoch: [2/10][ 500/1563]\tLoss: 1.9726 (2.1052 on avg)\n",
            "Epoch: [2/10][ 600/1563]\tLoss: 1.9321 (2.0883 on avg)\n",
            "Epoch: [2/10][ 700/1563]\tLoss: 1.9989 (2.0683 on avg)\n",
            "Epoch: [2/10][ 800/1563]\tLoss: 1.8824 (2.0514 on avg)\n",
            "Epoch: [2/10][ 900/1563]\tLoss: 1.9118 (2.0352 on avg)\n",
            "Epoch: [2/10][1000/1563]\tLoss: 1.8489 (2.0204 on avg)\n",
            "Epoch: [2/10][1100/1563]\tLoss: 1.7942 (2.0070 on avg)\n",
            "Epoch: [2/10][1200/1563]\tLoss: 1.6688 (1.9946 on avg)\n",
            "Epoch: [2/10][1300/1563]\tLoss: 1.7534 (1.9807 on avg)\n",
            "Epoch: [2/10][1400/1563]\tLoss: 1.8219 (1.9665 on avg)\n",
            "Epoch: [2/10][1500/1563]\tLoss: 1.5943 (1.9531 on avg)\n",
            "\n",
            "* Epoch: [2/10]\tTrain loss: 1.944\n",
            "\n",
            "Test (on val set): [2/10][   0/313]\tLoss: 1.8622 (1.8622 on avg)\tTop-1 err: 75.0000 (75.0000 on avg)\tTop-5 err: 18.7500 (18.7500 on avg)\n",
            "Test (on val set): [2/10][ 100/313]\tLoss: 1.6416 (1.7098 on avg)\tTop-1 err: 65.6250 (64.1399 on avg)\tTop-5 err: 0.0000 (11.6955 on avg)\n",
            "Test (on val set): [2/10][ 200/313]\tLoss: 1.8787 (1.7113 on avg)\tTop-1 err: 65.6250 (64.1480 on avg)\tTop-5 err: 18.7500 (11.3806 on avg)\n",
            "Test (on val set): [2/10][ 300/313]\tLoss: 1.6057 (1.7117 on avg)\tTop-1 err: 59.3750 (63.7874 on avg)\tTop-5 err: 3.1250 (11.5345 on avg)\n",
            "\n",
            "* Epoch: [2/10]\tTest loss: 1.711\tTop-1 err: 63.720\tTop-5 err: 11.500\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 25.52 1.93 \n",
            "\n",
            "Epoch: [3/10][   0/1563]\tLoss: 1.5268 (1.5268 on avg)\n",
            "Epoch: [3/10][ 100/1563]\tLoss: 1.7799 (1.7326 on avg)\n",
            "Epoch: [3/10][ 200/1563]\tLoss: 1.6511 (1.7242 on avg)\n",
            "Epoch: [3/10][ 300/1563]\tLoss: 1.6407 (1.7126 on avg)\n",
            "Epoch: [3/10][ 400/1563]\tLoss: 1.6817 (1.7056 on avg)\n",
            "Epoch: [3/10][ 500/1563]\tLoss: 1.7617 (1.7040 on avg)\n",
            "Epoch: [3/10][ 600/1563]\tLoss: 1.4952 (1.6999 on avg)\n",
            "Epoch: [3/10][ 700/1563]\tLoss: 1.2530 (1.6854 on avg)\n",
            "Epoch: [3/10][ 800/1563]\tLoss: 1.7255 (1.6747 on avg)\n",
            "Epoch: [3/10][ 900/1563]\tLoss: 1.4971 (1.6692 on avg)\n",
            "Epoch: [3/10][1000/1563]\tLoss: 1.5528 (1.6600 on avg)\n",
            "Epoch: [3/10][1100/1563]\tLoss: 1.4922 (1.6525 on avg)\n",
            "Epoch: [3/10][1200/1563]\tLoss: 1.6528 (1.6452 on avg)\n",
            "Epoch: [3/10][1300/1563]\tLoss: 1.7590 (1.6379 on avg)\n",
            "Epoch: [3/10][1400/1563]\tLoss: 1.3679 (1.6283 on avg)\n",
            "Epoch: [3/10][1500/1563]\tLoss: 1.6741 (1.6218 on avg)\n",
            "\n",
            "* Epoch: [3/10]\tTrain loss: 1.619\n",
            "\n",
            "Test (on val set): [3/10][   0/313]\tLoss: 1.5810 (1.5810 on avg)\tTop-1 err: 62.5000 (62.5000 on avg)\tTop-5 err: 6.2500 (6.2500 on avg)\n",
            "Test (on val set): [3/10][ 100/313]\tLoss: 1.5154 (1.4852 on avg)\tTop-1 err: 53.1250 (54.3007 on avg)\tTop-5 err: 6.2500 (7.6114 on avg)\n",
            "Test (on val set): [3/10][ 200/313]\tLoss: 1.5249 (1.4785 on avg)\tTop-1 err: 53.1250 (54.2910 on avg)\tTop-5 err: 12.5000 (7.7270 on avg)\n",
            "Test (on val set): [3/10][ 300/313]\tLoss: 1.4373 (1.4778 on avg)\tTop-1 err: 46.8750 (54.3086 on avg)\tTop-5 err: 9.3750 (7.6308 on avg)\n",
            "\n",
            "* Epoch: [3/10]\tTest loss: 1.475\tTop-1 err: 54.160\tTop-5 err: 7.630\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 25.52 1.93 \n",
            "\n",
            "Epoch: [4/10][   0/1563]\tLoss: 1.3508 (1.3508 on avg)\n",
            "Epoch: [4/10][ 100/1563]\tLoss: 1.3956 (1.4993 on avg)\n",
            "Epoch: [4/10][ 200/1563]\tLoss: 1.5907 (1.5138 on avg)\n",
            "Epoch: [4/10][ 300/1563]\tLoss: 1.6798 (1.5051 on avg)\n",
            "Epoch: [4/10][ 400/1563]\tLoss: 1.5596 (1.4929 on avg)\n",
            "Epoch: [4/10][ 500/1563]\tLoss: 1.7300 (1.4852 on avg)\n",
            "Epoch: [4/10][ 600/1563]\tLoss: 1.5972 (1.4816 on avg)\n",
            "Epoch: [4/10][ 700/1563]\tLoss: 1.2562 (1.4713 on avg)\n",
            "Epoch: [4/10][ 800/1563]\tLoss: 1.7926 (1.4663 on avg)\n",
            "Epoch: [4/10][ 900/1563]\tLoss: 1.0524 (1.4612 on avg)\n",
            "Epoch: [4/10][1000/1563]\tLoss: 1.3479 (1.4552 on avg)\n",
            "Epoch: [4/10][1100/1563]\tLoss: 1.2636 (1.4475 on avg)\n",
            "Epoch: [4/10][1200/1563]\tLoss: 1.2566 (1.4422 on avg)\n",
            "Epoch: [4/10][1300/1563]\tLoss: 1.3178 (1.4337 on avg)\n",
            "Epoch: [4/10][1400/1563]\tLoss: 1.3202 (1.4263 on avg)\n",
            "Epoch: [4/10][1500/1563]\tLoss: 1.0975 (1.4179 on avg)\n",
            "\n",
            "* Epoch: [4/10]\tTrain loss: 1.415\n",
            "\n",
            "Test (on val set): [4/10][   0/313]\tLoss: 1.0506 (1.0506 on avg)\tTop-1 err: 43.7500 (43.7500 on avg)\tTop-5 err: 0.0000 (0.0000 on avg)\n",
            "Test (on val set): [4/10][ 100/313]\tLoss: 1.1880 (1.2881 on avg)\tTop-1 err: 37.5000 (48.1745 on avg)\tTop-5 err: 6.2500 (5.4146 on avg)\n",
            "Test (on val set): [4/10][ 200/313]\tLoss: 1.3399 (1.2921 on avg)\tTop-1 err: 50.0000 (48.3831 on avg)\tTop-5 err: 9.3750 (5.0529 on avg)\n",
            "Test (on val set): [4/10][ 300/313]\tLoss: 1.6983 (1.2838 on avg)\tTop-1 err: 50.0000 (47.7056 on avg)\tTop-5 err: 9.3750 (5.0353 on avg)\n",
            "\n",
            "* Epoch: [4/10]\tTest loss: 1.280\tTop-1 err: 47.560\tTop-5 err: 5.010\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 25.52 1.93 \n",
            "\n",
            "Epoch: [5/10][   0/1563]\tLoss: 1.1209 (1.1209 on avg)\n",
            "Epoch: [5/10][ 100/1563]\tLoss: 1.5409 (1.2976 on avg)\n",
            "Epoch: [5/10][ 200/1563]\tLoss: 1.3000 (1.3022 on avg)\n",
            "Epoch: [5/10][ 300/1563]\tLoss: 1.3760 (1.2991 on avg)\n",
            "Epoch: [5/10][ 400/1563]\tLoss: 1.1170 (1.2956 on avg)\n",
            "Epoch: [5/10][ 500/1563]\tLoss: 1.2048 (1.2963 on avg)\n",
            "Epoch: [5/10][ 600/1563]\tLoss: 1.4967 (1.2877 on avg)\n",
            "Epoch: [5/10][ 700/1563]\tLoss: 1.2346 (1.2812 on avg)\n",
            "Epoch: [5/10][ 800/1563]\tLoss: 1.1373 (1.2759 on avg)\n",
            "Epoch: [5/10][ 900/1563]\tLoss: 1.2770 (1.2697 on avg)\n",
            "Epoch: [5/10][1000/1563]\tLoss: 1.2336 (1.2673 on avg)\n",
            "Epoch: [5/10][1100/1563]\tLoss: 0.9502 (1.2634 on avg)\n",
            "Epoch: [5/10][1200/1563]\tLoss: 1.2692 (1.2571 on avg)\n",
            "Epoch: [5/10][1300/1563]\tLoss: 1.3728 (1.2547 on avg)\n",
            "Epoch: [5/10][1400/1563]\tLoss: 1.2881 (1.2502 on avg)\n",
            "Epoch: [5/10][1500/1563]\tLoss: 1.1330 (1.2470 on avg)\n",
            "\n",
            "* Epoch: [5/10]\tTrain loss: 1.246\n",
            "\n",
            "Test (on val set): [5/10][   0/313]\tLoss: 1.3109 (1.3109 on avg)\tTop-1 err: 34.3750 (34.3750 on avg)\tTop-5 err: 12.5000 (12.5000 on avg)\n",
            "Test (on val set): [5/10][ 100/313]\tLoss: 1.1314 (1.2150 on avg)\tTop-1 err: 43.7500 (43.7809 on avg)\tTop-5 err: 9.3750 (5.0124 on avg)\n",
            "Test (on val set): [5/10][ 200/313]\tLoss: 1.1167 (1.2017 on avg)\tTop-1 err: 50.0000 (43.1748 on avg)\tTop-5 err: 3.1250 (4.8663 on avg)\n",
            "Test (on val set): [5/10][ 300/313]\tLoss: 1.1801 (1.2014 on avg)\tTop-1 err: 43.7500 (43.0233 on avg)\tTop-5 err: 3.1250 (4.6823 on avg)\n",
            "\n",
            "* Epoch: [5/10]\tTest loss: 1.203\tTop-1 err: 43.070\tTop-5 err: 4.730\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 25.52 1.93 \n",
            "\n",
            "Epoch: [6/10][   0/1563]\tLoss: 1.1982 (1.1982 on avg)\n",
            "Epoch: [6/10][ 100/1563]\tLoss: 1.2622 (1.1743 on avg)\n",
            "Epoch: [6/10][ 200/1563]\tLoss: 1.0258 (1.1546 on avg)\n",
            "Epoch: [6/10][ 300/1563]\tLoss: 1.0295 (1.1607 on avg)\n",
            "Epoch: [6/10][ 400/1563]\tLoss: 1.1500 (1.1556 on avg)\n",
            "Epoch: [6/10][ 500/1563]\tLoss: 1.0848 (1.1584 on avg)\n",
            "Epoch: [6/10][ 600/1563]\tLoss: 1.4234 (1.1516 on avg)\n",
            "Epoch: [6/10][ 700/1563]\tLoss: 1.0333 (1.1465 on avg)\n",
            "Epoch: [6/10][ 800/1563]\tLoss: 0.8490 (1.1427 on avg)\n",
            "Epoch: [6/10][ 900/1563]\tLoss: 1.0315 (1.1375 on avg)\n",
            "Epoch: [6/10][1000/1563]\tLoss: 1.2139 (1.1369 on avg)\n",
            "Epoch: [6/10][1100/1563]\tLoss: 1.2383 (1.1319 on avg)\n",
            "Epoch: [6/10][1200/1563]\tLoss: 1.2107 (1.1287 on avg)\n",
            "Epoch: [6/10][1300/1563]\tLoss: 1.3327 (1.1253 on avg)\n",
            "Epoch: [6/10][1400/1563]\tLoss: 0.9010 (1.1205 on avg)\n",
            "Epoch: [6/10][1500/1563]\tLoss: 1.1677 (1.1155 on avg)\n",
            "\n",
            "* Epoch: [6/10]\tTrain loss: 1.114\n",
            "\n",
            "Test (on val set): [6/10][   0/313]\tLoss: 1.0063 (1.0063 on avg)\tTop-1 err: 37.5000 (37.5000 on avg)\tTop-5 err: 0.0000 (0.0000 on avg)\n",
            "Test (on val set): [6/10][ 100/313]\tLoss: 1.2653 (1.0263 on avg)\tTop-1 err: 50.0000 (36.7265 on avg)\tTop-5 err: 3.1250 (3.1559 on avg)\n",
            "Test (on val set): [6/10][ 200/313]\tLoss: 1.0453 (1.0263 on avg)\tTop-1 err: 34.3750 (36.4272 on avg)\tTop-5 err: 3.1250 (3.3427 on avg)\n",
            "Test (on val set): [6/10][ 300/313]\tLoss: 0.8885 (1.0324 on avg)\tTop-1 err: 31.2500 (36.8355 on avg)\tTop-5 err: 3.1250 (3.2600 on avg)\n",
            "\n",
            "* Epoch: [6/10]\tTest loss: 1.033\tTop-1 err: 36.900\tTop-5 err: 3.340\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 25.52 1.93 \n",
            "\n",
            "Epoch: [7/10][   0/1563]\tLoss: 1.0537 (1.0537 on avg)\n",
            "Epoch: [7/10][ 100/1563]\tLoss: 1.2176 (1.0322 on avg)\n",
            "Epoch: [7/10][ 200/1563]\tLoss: 1.4438 (1.0394 on avg)\n",
            "Epoch: [7/10][ 300/1563]\tLoss: 1.2574 (1.0429 on avg)\n",
            "Epoch: [7/10][ 400/1563]\tLoss: 0.9520 (1.0366 on avg)\n",
            "Epoch: [7/10][ 500/1563]\tLoss: 0.8166 (1.0331 on avg)\n",
            "Epoch: [7/10][ 600/1563]\tLoss: 1.1208 (1.0246 on avg)\n",
            "Epoch: [7/10][ 700/1563]\tLoss: 1.0549 (1.0199 on avg)\n",
            "Epoch: [7/10][ 800/1563]\tLoss: 1.2509 (1.0169 on avg)\n",
            "Epoch: [7/10][ 900/1563]\tLoss: 1.1459 (1.0160 on avg)\n",
            "Epoch: [7/10][1000/1563]\tLoss: 1.2254 (1.0151 on avg)\n",
            "Epoch: [7/10][1100/1563]\tLoss: 1.1447 (1.0134 on avg)\n",
            "Epoch: [7/10][1200/1563]\tLoss: 1.1355 (1.0113 on avg)\n",
            "Epoch: [7/10][1300/1563]\tLoss: 0.9213 (1.0075 on avg)\n",
            "Epoch: [7/10][1400/1563]\tLoss: 0.9852 (1.0062 on avg)\n",
            "Epoch: [7/10][1500/1563]\tLoss: 1.0089 (1.0025 on avg)\n",
            "\n",
            "* Epoch: [7/10]\tTrain loss: 1.001\n",
            "\n",
            "Test (on val set): [7/10][   0/313]\tLoss: 1.1535 (1.1535 on avg)\tTop-1 err: 31.2500 (31.2500 on avg)\tTop-5 err: 3.1250 (3.1250 on avg)\n",
            "Test (on val set): [7/10][ 100/313]\tLoss: 0.6087 (0.9280 on avg)\tTop-1 err: 15.6250 (33.3540 on avg)\tTop-5 err: 0.0000 (2.9394 on avg)\n",
            "Test (on val set): [7/10][ 200/313]\tLoss: 1.0455 (0.9390 on avg)\tTop-1 err: 31.2500 (33.3333 on avg)\tTop-5 err: 3.1250 (2.7985 on avg)\n",
            "Test (on val set): [7/10][ 300/313]\tLoss: 0.8531 (0.9334 on avg)\tTop-1 err: 31.2500 (32.7969 on avg)\tTop-5 err: 3.1250 (2.6682 on avg)\n",
            "\n",
            "* Epoch: [7/10]\tTest loss: 0.934\tTop-1 err: 32.900\tTop-5 err: 2.660\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 25.52 1.93 \n",
            "\n",
            "Epoch: [8/10][   0/1563]\tLoss: 1.0919 (1.0919 on avg)\n",
            "Epoch: [8/10][ 100/1563]\tLoss: 0.8693 (0.9330 on avg)\n",
            "Epoch: [8/10][ 200/1563]\tLoss: 0.7516 (0.9313 on avg)\n",
            "Epoch: [8/10][ 300/1563]\tLoss: 0.9179 (0.9292 on avg)\n",
            "Epoch: [8/10][ 400/1563]\tLoss: 0.7841 (0.9279 on avg)\n",
            "Epoch: [8/10][ 500/1563]\tLoss: 0.9722 (0.9297 on avg)\n",
            "Epoch: [8/10][ 600/1563]\tLoss: 0.7618 (0.9280 on avg)\n",
            "Epoch: [8/10][ 700/1563]\tLoss: 0.9346 (0.9260 on avg)\n",
            "Epoch: [8/10][ 800/1563]\tLoss: 0.6602 (0.9252 on avg)\n",
            "Epoch: [8/10][ 900/1563]\tLoss: 0.8811 (0.9232 on avg)\n",
            "Epoch: [8/10][1000/1563]\tLoss: 0.9952 (0.9215 on avg)\n",
            "Epoch: [8/10][1100/1563]\tLoss: 0.6224 (0.9169 on avg)\n",
            "Epoch: [8/10][1200/1563]\tLoss: 1.1018 (0.9161 on avg)\n",
            "Epoch: [8/10][1300/1563]\tLoss: 0.8495 (0.9152 on avg)\n",
            "Epoch: [8/10][1400/1563]\tLoss: 0.7195 (0.9128 on avg)\n",
            "Epoch: [8/10][1500/1563]\tLoss: 0.8507 (0.9099 on avg)\n",
            "\n",
            "* Epoch: [8/10]\tTrain loss: 0.909\n",
            "\n",
            "Test (on val set): [8/10][   0/313]\tLoss: 0.9433 (0.9433 on avg)\tTop-1 err: 34.3750 (34.3750 on avg)\tTop-5 err: 9.3750 (9.3750 on avg)\n",
            "Test (on val set): [8/10][ 100/313]\tLoss: 0.9990 (0.8715 on avg)\tTop-1 err: 31.2500 (30.8787 on avg)\tTop-5 err: 3.1250 (2.3515 on avg)\n",
            "Test (on val set): [8/10][ 200/313]\tLoss: 0.8578 (0.8619 on avg)\tTop-1 err: 31.2500 (30.4260 on avg)\tTop-5 err: 0.0000 (2.3787 on avg)\n",
            "Test (on val set): [8/10][ 300/313]\tLoss: 0.9072 (0.8666 on avg)\tTop-1 err: 34.3750 (30.5336 on avg)\tTop-5 err: 0.0000 (2.4294 on avg)\n",
            "\n",
            "* Epoch: [8/10]\tTest loss: 0.866\tTop-1 err: 30.630\tTop-5 err: 2.420\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 25.52 1.93 \n",
            "\n",
            "Epoch: [9/10][   0/1563]\tLoss: 0.9622 (0.9622 on avg)\n",
            "Epoch: [9/10][ 100/1563]\tLoss: 0.7739 (0.8358 on avg)\n",
            "Epoch: [9/10][ 200/1563]\tLoss: 0.6275 (0.8453 on avg)\n",
            "Epoch: [9/10][ 300/1563]\tLoss: 0.6396 (0.8554 on avg)\n",
            "Epoch: [9/10][ 400/1563]\tLoss: 0.6374 (0.8495 on avg)\n",
            "Epoch: [9/10][ 500/1563]\tLoss: 0.9118 (0.8422 on avg)\n",
            "Epoch: [9/10][ 600/1563]\tLoss: 0.8717 (0.8438 on avg)\n",
            "Epoch: [9/10][ 700/1563]\tLoss: 0.9155 (0.8446 on avg)\n",
            "Epoch: [9/10][ 800/1563]\tLoss: 0.9216 (0.8429 on avg)\n",
            "Epoch: [9/10][ 900/1563]\tLoss: 0.5681 (0.8385 on avg)\n",
            "Epoch: [9/10][1000/1563]\tLoss: 0.6340 (0.8365 on avg)\n",
            "Epoch: [9/10][1100/1563]\tLoss: 0.8612 (0.8385 on avg)\n",
            "Epoch: [9/10][1200/1563]\tLoss: 0.8535 (0.8341 on avg)\n",
            "Epoch: [9/10][1300/1563]\tLoss: 1.0126 (0.8340 on avg)\n",
            "Epoch: [9/10][1400/1563]\tLoss: 0.5261 (0.8314 on avg)\n",
            "Epoch: [9/10][1500/1563]\tLoss: 0.6951 (0.8282 on avg)\n",
            "\n",
            "* Epoch: [9/10]\tTrain loss: 0.826\n",
            "\n",
            "Test (on val set): [9/10][   0/313]\tLoss: 0.7570 (0.7570 on avg)\tTop-1 err: 28.1250 (28.1250 on avg)\tTop-5 err: 3.1250 (3.1250 on avg)\n",
            "Test (on val set): [9/10][ 100/313]\tLoss: 0.6412 (0.8184 on avg)\tTop-1 err: 37.5000 (28.5582 on avg)\tTop-5 err: 0.0000 (2.1658 on avg)\n",
            "Test (on val set): [9/10][ 200/313]\tLoss: 0.7984 (0.8331 on avg)\tTop-1 err: 25.0000 (29.3999 on avg)\tTop-5 err: 3.1250 (2.1766 on avg)\n",
            "Test (on val set): [9/10][ 300/313]\tLoss: 1.0620 (0.8431 on avg)\tTop-1 err: 37.5000 (29.7446 on avg)\tTop-5 err: 6.2500 (2.1699 on avg)\n",
            "\n",
            "* Epoch: [9/10]\tTest loss: 0.841\tTop-1 err: 29.740\tTop-5 err: 2.140\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 25.52 1.93 \n",
            "\n",
            "Epoch: [10/10][   0/1563]\tLoss: 1.2193 (1.2193 on avg)\n",
            "Epoch: [10/10][ 100/1563]\tLoss: 0.6810 (0.7999 on avg)\n",
            "Epoch: [10/10][ 200/1563]\tLoss: 0.7812 (0.7968 on avg)\n",
            "Epoch: [10/10][ 300/1563]\tLoss: 0.7276 (0.7856 on avg)\n",
            "Epoch: [10/10][ 400/1563]\tLoss: 1.0128 (0.7764 on avg)\n",
            "Epoch: [10/10][ 500/1563]\tLoss: 0.8320 (0.7804 on avg)\n",
            "Epoch: [10/10][ 600/1563]\tLoss: 0.8027 (0.7804 on avg)\n",
            "Epoch: [10/10][ 700/1563]\tLoss: 0.9889 (0.7737 on avg)\n",
            "Epoch: [10/10][ 800/1563]\tLoss: 0.7537 (0.7745 on avg)\n",
            "Epoch: [10/10][ 900/1563]\tLoss: 0.9533 (0.7704 on avg)\n",
            "Epoch: [10/10][1000/1563]\tLoss: 0.3605 (0.7719 on avg)\n",
            "Epoch: [10/10][1100/1563]\tLoss: 0.9475 (0.7695 on avg)\n",
            "Epoch: [10/10][1200/1563]\tLoss: 0.4729 (0.7689 on avg)\n",
            "Epoch: [10/10][1300/1563]\tLoss: 1.1558 (0.7699 on avg)\n",
            "Epoch: [10/10][1400/1563]\tLoss: 0.6466 (0.7689 on avg)\n",
            "Epoch: [10/10][1500/1563]\tLoss: 0.9308 (0.7667 on avg)\n",
            "\n",
            "* Epoch: [10/10]\tTrain loss: 0.766\n",
            "\n",
            "Test (on val set): [10/10][   0/313]\tLoss: 0.5412 (0.5412 on avg)\tTop-1 err: 18.7500 (18.7500 on avg)\tTop-5 err: 3.1250 (3.1250 on avg)\n",
            "Test (on val set): [10/10][ 100/313]\tLoss: 0.6244 (0.7726 on avg)\tTop-1 err: 18.7500 (27.0111 on avg)\tTop-5 err: 3.1250 (2.1968 on avg)\n",
            "Test (on val set): [10/10][ 200/313]\tLoss: 1.0715 (0.7641 on avg)\tTop-1 err: 37.5000 (26.7879 on avg)\tTop-5 err: 6.2500 (1.9745 on avg)\n",
            "Test (on val set): [10/10][ 300/313]\tLoss: 0.8404 (0.7781 on avg)\tTop-1 err: 31.2500 (27.0972 on avg)\tTop-5 err: 3.1250 (2.1179 on avg)\n",
            "\n",
            "* Epoch: [10/10]\tTest loss: 0.776\tTop-1 err: 26.990\tTop-5 err: 2.130\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 25.52 1.93 \n",
            "\n",
            "Best error rate (top-1 and top-5 error): 25.52 1.93\n",
            "Learning Rate: 0.001\n"
          ]
        }
      ],
      "source": [
        "# Run the main function.\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # select a model to train here (CIFAR10Net or MNISTNet)\n",
        "    model = CIFAR10Net()\n",
        "\n",
        "    # move to GPU\n",
        "    model.to(device)\n",
        "\n",
        "    # select number of epochs\n",
        "    epochs = 10\n",
        "\n",
        "    # get criterion and optimizer\n",
        "    criterion, optimizer = get_crit_and_opt(model)\n",
        "\n",
        "    # epoch loop\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        # train for one epoch\n",
        "        train_loss = train(\n",
        "          train_loader,\n",
        "          model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          epoch,\n",
        "          epochs\n",
        "        )\n",
        "\n",
        "        # evaluate on validation set\n",
        "        err1, err5, val_loss = validate(\n",
        "          val_loader,\n",
        "          model,\n",
        "          criterion,\n",
        "          epoch,\n",
        "          epochs\n",
        "        )\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = err1 <= best_err1\n",
        "        best_err1 = min(err1, best_err1)\n",
        "        if is_best:\n",
        "            best_err5 = err5\n",
        "\n",
        "        print('Current best error rate (top-1 and top-5 error):', best_err1, best_err5, '\\n')\n",
        "    print('Best error rate (top-1 and top-5 error):', best_err1, best_err5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhPedCkKdiQx",
        "outputId": "2ef73aaf-90a3-43fa-97bb-e5be4eb4cd9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.74      0.69      0.72      1000\n",
            "        bird       0.90      0.75      0.82      1000\n",
            "   vegetable       0.59      0.54      0.56      1000\n",
            "         dog       0.50      0.26      0.34      1000\n",
            "         cat       0.71      0.52      0.60      1000\n",
            "         car       0.44      0.78      0.56      1000\n",
            "       fruit       0.63      0.88      0.73      1000\n",
            "       train       0.69      0.75      0.72      1000\n",
            "      rabbit       0.91      0.70      0.79      1000\n",
            "        baby       0.75      0.79      0.77      1000\n",
            "\n",
            "    accuracy                           0.67     10000\n",
            "   macro avg       0.69      0.67      0.66     10000\n",
            "weighted avg       0.69      0.67      0.66     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a classification report for one model [10 pts]\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation for efficiency\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        # Move tensors to the appropriate device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass to get the model's predictions\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Convert outputs to predicted class by taking the index with the maximum score in each output row\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Append true and predicted labels to lists\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to arrays for compatibility with classification_report\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "#___________________________________________________________________________________________________________________________\n",
        "\n",
        "# Assuming you have 10 classes for CIFAR10, and their names as follows:\n",
        "target_names = ['airplane', 'bird', 'vegetable', 'dog', 'cat',\n",
        "                'car', 'fruit', 'train', 'rabbit', 'baby']\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_true, y_pred, target_names=target_names)\n",
        "\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHnTRaiulQbo"
      },
      "source": [
        "6. Evaluation and Analysis\n",
        "Evaluate the Model: Test the model on both normal and adversarially modified images.\n",
        "Plot Results: Use Matplotlib to visualize the training and validation results.\n",
        "\n",
        "7. Documentation and Saving Work\n",
        "Document Your Work: Utilize Markdown cells in Google Colab to describe each part of your process.\n",
        "Save Your Model and Data: Ensure to save your trained model and any important data for later use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-N9zTgt0NOS"
      },
      "source": [
        "### **Error Injection Testing**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-RgKNpDCemr"
      },
      "source": [
        "In models that use dropout, different neurons are randomly dropped in each forward pass during training. This means that with each batch during an epoch, and indeed across epochs, different sets of neurons might be turned off (i.e., their activations are set to zero or replaced with noise in the case of your NoisyDropout). This randomness is key to how dropout enhances the generalization capabilities of neural networks by preventing them from being overly reliant on any particular neuron or feature detector, thereby reducing overfitting.\n",
        "\n",
        "How Dropout Works with Noise\n",
        "When using the NoisyDropout class that you've implemented, here's what happens during each training iteration (not just each epoch):\n",
        "\n",
        "Binary Mask Generation: A random binary mask is generated where the probability of a neuron being zeroed out (dropped) is determined by the dropout probability p. This mask is different for each forward pass.\n",
        "\n",
        "Noise Application: For the neurons that are dropped (where the mask is True), random Gaussian noise is added. This noise is generated anew each time, so its values are different in each forward pass.\n",
        "\n",
        "Forward Pass Application: The forward method of NoisyDropout applies this mask and noise to the input tensor x, modifying the activations accordingly.\n",
        "\n",
        "Ensuring Neurons Receive New Noise Each Epoch\n",
        "The process you've set up with NoisyDropout inherently ensures that each time an epoch progresses and for each batch within that epoch, the dropped neurons receive new noise. This is due to the stochastic nature of both the dropout mask and the noise generation occurring within the forward method of the dropout layer. Each call to the forward method generates a new dropout mask and new noise based on the current state of the random number generator.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqYJQ9jv2GXq"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import CIFAR10, MNIST\n",
        "from torchvision.transforms.v2 import ToTensor\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0isqal8a2LHE",
        "outputId": "cb6b1fa6-e5cb-4c10-c21e-f09ff04ff610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-10 transform - three channels, normalize with 3 means and 3 SDs\n",
        "cifar_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# CIFAR10 data\n",
        "train_dataset = CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")\n",
        "\n",
        "val_dataset = CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=cifar_transform\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6oAZ0Zb2r7e"
      },
      "outputs": [],
      "source": [
        "# dataloaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-BvSplSdN4h",
        "outputId": "5baa422b-8cbc-4c77-88f1-b643f50b7159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed standard deviation of the dataset: tensor(0.1529)\n"
          ]
        }
      ],
      "source": [
        "#Compute Standard Deviation of the Dataset\n",
        "\n",
        "def compute_dataset_std(train_loader):\n",
        "    # We square sum the standard deviations of each batch to compute the total variance, then take the square root.\n",
        "    var_sum = 0\n",
        "    count = 0\n",
        "    for data, _ in train_loader:\n",
        "        var_sum += data.var([0, 2, 3], unbiased=False).sum()\n",
        "        count += data.size(0)\n",
        "    total_variance = var_sum / count\n",
        "    dataset_std = torch.sqrt(total_variance)\n",
        "    return dataset_std.mean()\n",
        "\n",
        "dataset_std = compute_dataset_std(train_loader)\n",
        "print(\"Computed standard deviation of the dataset:\", dataset_std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CFyEDhD2x2k"
      },
      "outputs": [],
      "source": [
        "# Loss fuction and optimizer\n",
        "def get_crit_and_opt(net):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    return criterion, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFivOj5IEAsW"
      },
      "source": [
        "There are two primary parameters that control the behavior of the dropout and the noise:\n",
        "\n",
        "p (Dropout Probability): This parameter specifies the probability with which each neuron is dropped (i.e., turned off). Setting this parameter to a higher value increases the number of neurons that are dropped during each forward pass.\n",
        "\n",
        "std_dev (Standard Deviation of Noise): This parameter controls the amount of noise added to the neurons that are dropped. A higher standard deviation results in greater noise being added, which increases the variability and randomness introduced into the model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9jGmMFu2R2s"
      },
      "outputs": [],
      "source": [
        "# Noisy Dropout Class Definition\n",
        "class NoisyDropout(nn.Module):\n",
        "    def __init__(self, p=0.25, std_dev=0.1529):\n",
        "        super(NoisyDropout, self).__init__()\n",
        "        self.p = p  # Dropout probability\n",
        "        self.std_dev = std_dev  # Standard deviation of the Gaussian noise\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training:\n",
        "            return x # During evaluation, dropout is bypassed entirely.\n",
        "        else:\n",
        "            # Create a dropout mask with the same shape as the input\n",
        "            # Generate a random dropout mask that has 'True' with probability 'p' (dropout rate)\n",
        "            dropout_mask = torch.rand_like(x) < self.p\n",
        "\n",
        "            # Generate Gaussian noise with the same shape as the input\n",
        "            # Generate Gaussian noise with the same shape as the input tensor.\n",
        "            noise = torch.randn_like(x) * self.std_dev\n",
        "\n",
        "            # Apply noise where the dropout mask is True, otherwise keep the neuron's original value\n",
        "            # Apply noise where the dropout mask is 'True'; keep original x values where 'False'.\n",
        "            return torch.where(dropout_mask, noise, x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZMzwtS5Eq0t"
      },
      "source": [
        "Adjusting the Noise Rate or Ratio\n",
        "To adjust the rate at which noise affects the model, you can modify the p and std_dev parameters when you instantiate the NoisyDropout layer within your network. Here's how you can do this in a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVOe_qNYhZOQ"
      },
      "outputs": [],
      "source": [
        "class CIFAR10Net(nn.Module):\n",
        "    def __init__(self, dropout_p=0.25, noise_std=0.1529):\n",
        "        super(CIFAR10Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # NoisyDropout layers\n",
        "        #self.noisy_dropout1 = NoisyDropout(p=dropout_p, std_dev=noise_std)\n",
        "        #self.noisy_dropout2 = NoisyDropout(p=dropout_p, std_dev=noise_std)\n",
        "        self.noisy_dropout3 = NoisyDropout(p=dropout_p, std_dev=noise_std)\n",
        "        self.noisy_dropout4 = NoisyDropout(p=dropout_p, std_dev=noise_std)\n",
        "\n",
        "        # Dummy input to determine size of the output from final conv layer\n",
        "        dummy_input = torch.autograd.Variable(torch.zeros(1, 3, 32, 32))\n",
        "        dummy_output = self.pool(self.conv4(self.pool(self.conv3(self.pool(self.conv2(self.pool(self.conv1(dummy_input))))))))\n",
        "        self.final_feature_map_size = dummy_output.size(-1) * dummy_output.size(-2) * dummy_output.size(-3)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.final_feature_map_size, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.pool(self.conv1(x)))\n",
        "        #x = self.noisy_dropout1(x)  # Apply NoisyDropout after first pooling\n",
        "        x = F.relu(self.pool(self.conv2(x)))\n",
        "        #x = self.noisy_dropout2(x)  # Apply NoisyDropout after second pooling\n",
        "        x = F.relu(self.pool(self.conv3(x)))\n",
        "        x = self.noisy_dropout3(x)  # Apply NoisyDropout after third pooling\n",
        "        x = F.relu(self.pool(self.conv4(x)))\n",
        "        x = self.noisy_dropout4(x)  # Apply NoisyDropout after fourth pooling\n",
        "\n",
        "        x = x.view(-1, self.final_feature_map_size)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1jD22bX3NDs"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "\n",
        "    \"\"\"Computes and stores an average and current value.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-RS6Mw33RN_"
      },
      "outputs": [],
      "source": [
        "def error_rate(output, target, topk=(1,)):\n",
        "\n",
        "    \"\"\"Computes the top-k error rate for the specified values of k.\"\"\"\n",
        "\n",
        "    maxk = max(topk) # largest k we'll need to work with\n",
        "    batch_size = target.size(0) # determine batch size\n",
        "\n",
        "    # get maxk best predictions for each item in the batch, both values and indices\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "\n",
        "    # reshape predictions and targets and compare them element-wise\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk: # for each top-k accuracy we want\n",
        "\n",
        "        # num correct\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        # num incorrect\n",
        "        wrong_k = batch_size - correct_k\n",
        "        # as a percentage\n",
        "        res.append(wrong_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92YKsiBj3Wqe"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwjMnuwW3emc"
      },
      "outputs": [],
      "source": [
        "# training function - 1 epoch\n",
        "def train(\n",
        "    train_loader,\n",
        "    model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    epoch,\n",
        "    epochs,\n",
        "    print_freq = 100,\n",
        "    verbose = True\n",
        "):\n",
        "\n",
        "    # track average and worst losses\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # set training mode\n",
        "    model.train()\n",
        "\n",
        "    # iterate over data - automatically shuffled\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        # put batch of image tensors on GPU\n",
        "        images = images.to(device)\n",
        "        # put batch of label tensors on GPU\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # model output\n",
        "        outputs = model(images)\n",
        "\n",
        "        # loss computation\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # back propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # update meter with the value of the loss once for each item in the batch\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "\n",
        "        # logging during epoch\n",
        "        if i % print_freq == 0 and verbose == True:\n",
        "            print(\n",
        "                f'Epoch: [{epoch+1}/{epochs}][{i:4}/{len(train_loader)}]\\t'\n",
        "                f'Loss: {losses.val:.4f} ({losses.avg:.4f} on avg)'\n",
        "            )\n",
        "\n",
        "    # log again at end of epoch\n",
        "    print(f'\\n* Epoch: [{epoch+1}/{epochs}]\\tTrain loss: {losses.avg:.3f}\\n')\n",
        "\n",
        "    return losses.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6EBea2S3_Iu"
      },
      "outputs": [],
      "source": [
        "# val function\n",
        "def validate(\n",
        "    val_loader,\n",
        "    model,\n",
        "    criterion,\n",
        "    epoch,\n",
        "    epochs,\n",
        "    print_freq = 100,\n",
        "    verbose = True\n",
        "):\n",
        "\n",
        "    # track average and worst losses and batch-wise top-1 and top-5 accuracies\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # set evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # iterate over data - automatically shuffled\n",
        "    for i, (images, labels) in enumerate(val_loader):\n",
        "\n",
        "        # put batch of image tensors on GPU\n",
        "        images = images.to(device)\n",
        "        # put batch of label tensors on GPU\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # model output\n",
        "        output = model(images)\n",
        "\n",
        "        # loss computation\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        # top-1 and top-5 accuracy on this batch\n",
        "        err1, err5, = error_rate(output.data, labels, topk=(1, 5))\n",
        "\n",
        "        # update meters with the value of the loss once for each item in the batch\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        # update meters with top-1 and top-5 accuracy on this batch once for each item in the batch\n",
        "        top1.update(err1.item(), images.size(0))\n",
        "        top5.update(err5.item(), images.size(0))\n",
        "\n",
        "        # logging during epoch\n",
        "        if i % print_freq == 0 and verbose == True:\n",
        "            print(\n",
        "                f'Test (on val set): [{epoch+1}/{epochs}][{i:4}/{len(val_loader)}]\\t'\n",
        "                f'Loss: {losses.val:.4f} ({losses.avg:.4f} on avg)\\t'\n",
        "                f'Top-1 err: {top1.val:.4f} ({top1.avg:.4f} on avg)\\t'\n",
        "                f'Top-5 err: {top5.val:.4f} ({top5.avg:.4f} on avg)'\n",
        "            )\n",
        "\n",
        "    # logging for end of epoch\n",
        "    print(\n",
        "        f'\\n* Epoch: [{epoch+1}/{epochs}]\\t'\n",
        "        f'Test loss: {losses.avg:.3f}\\t'\n",
        "        f'Top-1 err: {top1.avg:.3f}\\t'\n",
        "        f'Top-5 err: {top5.avg:.3f}\\n'\n",
        "    )\n",
        "\n",
        "    # avergae top-1 and top-5 accuracies batch-wise, and average loss batch-wise\n",
        "    return top1.avg, top5.avg, losses.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi60bXaq4Rt0"
      },
      "outputs": [],
      "source": [
        "# best error rates so far\n",
        "best_err1 = 100\n",
        "best_err5 = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cjg5HiYhxX9j",
        "outputId": "3e40a7e1-0cec-4c97-c53b-e4d200ca86ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1/10][   0/1563]\tLoss: 2.3011 (2.3011 on avg)\n",
            "Epoch: [1/10][ 100/1563]\tLoss: 2.3015 (2.3030 on avg)\n",
            "Epoch: [1/10][ 200/1563]\tLoss: 2.3021 (2.3029 on avg)\n",
            "Epoch: [1/10][ 300/1563]\tLoss: 2.3031 (2.3026 on avg)\n",
            "Epoch: [1/10][ 400/1563]\tLoss: 2.3003 (2.3024 on avg)\n",
            "Epoch: [1/10][ 500/1563]\tLoss: 2.3023 (2.3024 on avg)\n",
            "Epoch: [1/10][ 600/1563]\tLoss: 2.3080 (2.3024 on avg)\n",
            "Epoch: [1/10][ 700/1563]\tLoss: 2.3024 (2.3023 on avg)\n",
            "Epoch: [1/10][ 800/1563]\tLoss: 2.3002 (2.3022 on avg)\n",
            "Epoch: [1/10][ 900/1563]\tLoss: 2.2999 (2.3021 on avg)\n",
            "Epoch: [1/10][1000/1563]\tLoss: 2.3028 (2.3020 on avg)\n",
            "Epoch: [1/10][1100/1563]\tLoss: 2.2984 (2.3018 on avg)\n",
            "Epoch: [1/10][1200/1563]\tLoss: 2.2996 (2.3016 on avg)\n",
            "Epoch: [1/10][1300/1563]\tLoss: 2.3012 (2.3014 on avg)\n",
            "Epoch: [1/10][1400/1563]\tLoss: 2.2930 (2.3011 on avg)\n",
            "Epoch: [1/10][1500/1563]\tLoss: 2.3024 (2.3007 on avg)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "* Epoch: [1/10]\tTrain loss: 2.300\n",
            "\n",
            "Test (on val set): [1/10][   0/313]\tLoss: 2.2728 (2.2728 on avg)\tTop-1 err: 84.3750 (84.3750 on avg)\tTop-5 err: 46.8750 (46.8750 on avg)\n",
            "Test (on val set): [1/10][ 100/313]\tLoss: 2.2738 (2.2876 on avg)\tTop-1 err: 71.8750 (85.1176 on avg)\tTop-5 err: 21.8750 (45.5136 on avg)\n",
            "Test (on val set): [1/10][ 200/313]\tLoss: 2.2820 (2.2872 on avg)\tTop-1 err: 90.6250 (85.2767 on avg)\tTop-5 err: 37.5000 (45.3980 on avg)\n",
            "Test (on val set): [1/10][ 300/313]\tLoss: 2.2871 (2.2871 on avg)\tTop-1 err: 93.7500 (85.1640 on avg)\tTop-5 err: 56.2500 (45.2554 on avg)\n",
            "\n",
            "* Epoch: [1/10]\tTest loss: 2.287\tTop-1 err: 85.060\tTop-5 err: 45.220\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 32.97 3.07 \n",
            "\n",
            "Epoch: [2/10][   0/1563]\tLoss: 2.3012 (2.3012 on avg)\n",
            "Epoch: [2/10][ 100/1563]\tLoss: 2.2902 (2.2891 on avg)\n",
            "Epoch: [2/10][ 200/1563]\tLoss: 2.2798 (2.2849 on avg)\n",
            "Epoch: [2/10][ 300/1563]\tLoss: 2.2479 (2.2787 on avg)\n",
            "Epoch: [2/10][ 400/1563]\tLoss: 2.3239 (2.2712 on avg)\n",
            "Epoch: [2/10][ 500/1563]\tLoss: 2.1850 (2.2629 on avg)\n",
            "Epoch: [2/10][ 600/1563]\tLoss: 2.2178 (2.2517 on avg)\n",
            "Epoch: [2/10][ 700/1563]\tLoss: 2.1679 (2.2409 on avg)\n",
            "Epoch: [2/10][ 800/1563]\tLoss: 2.0968 (2.2313 on avg)\n",
            "Epoch: [2/10][ 900/1563]\tLoss: 2.1323 (2.2196 on avg)\n",
            "Epoch: [2/10][1000/1563]\tLoss: 2.1724 (2.2101 on avg)\n",
            "Epoch: [2/10][1100/1563]\tLoss: 2.1142 (2.1993 on avg)\n",
            "Epoch: [2/10][1200/1563]\tLoss: 2.0552 (2.1899 on avg)\n",
            "Epoch: [2/10][1300/1563]\tLoss: 2.0764 (2.1774 on avg)\n",
            "Epoch: [2/10][1400/1563]\tLoss: 2.0252 (2.1647 on avg)\n",
            "Epoch: [2/10][1500/1563]\tLoss: 1.9922 (2.1532 on avg)\n",
            "\n",
            "* Epoch: [2/10]\tTrain loss: 2.146\n",
            "\n",
            "Test (on val set): [2/10][   0/313]\tLoss: 1.7424 (1.7424 on avg)\tTop-1 err: 71.8750 (71.8750 on avg)\tTop-5 err: 21.8750 (21.8750 on avg)\n",
            "Test (on val set): [2/10][ 100/313]\tLoss: 1.7467 (1.9744 on avg)\tTop-1 err: 68.7500 (75.0000 on avg)\tTop-5 err: 6.2500 (17.9146 on avg)\n",
            "Test (on val set): [2/10][ 200/313]\tLoss: 1.7708 (1.9766 on avg)\tTop-1 err: 78.1250 (75.0777 on avg)\tTop-5 err: 15.6250 (17.7861 on avg)\n",
            "Test (on val set): [2/10][ 300/313]\tLoss: 1.7635 (1.9805 on avg)\tTop-1 err: 75.0000 (74.9896 on avg)\tTop-5 err: 12.5000 (18.1375 on avg)\n",
            "\n",
            "* Epoch: [2/10]\tTest loss: 1.979\tTop-1 err: 75.080\tTop-5 err: 18.020\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 32.97 3.07 \n",
            "\n",
            "Epoch: [3/10][   0/1563]\tLoss: 1.7848 (1.7848 on avg)\n",
            "Epoch: [3/10][ 100/1563]\tLoss: 1.9217 (1.9484 on avg)\n",
            "Epoch: [3/10][ 200/1563]\tLoss: 1.9187 (1.9375 on avg)\n",
            "Epoch: [3/10][ 300/1563]\tLoss: 1.9759 (1.9408 on avg)\n",
            "Epoch: [3/10][ 400/1563]\tLoss: 1.9303 (1.9330 on avg)\n",
            "Epoch: [3/10][ 500/1563]\tLoss: 1.9031 (1.9243 on avg)\n",
            "Epoch: [3/10][ 600/1563]\tLoss: 1.8398 (1.9152 on avg)\n",
            "Epoch: [3/10][ 700/1563]\tLoss: 1.8935 (1.9112 on avg)\n",
            "Epoch: [3/10][ 800/1563]\tLoss: 1.9274 (1.9046 on avg)\n",
            "Epoch: [3/10][ 900/1563]\tLoss: 1.7886 (1.8977 on avg)\n",
            "Epoch: [3/10][1000/1563]\tLoss: 1.8417 (1.8897 on avg)\n",
            "Epoch: [3/10][1100/1563]\tLoss: 1.6401 (1.8796 on avg)\n",
            "Epoch: [3/10][1200/1563]\tLoss: 1.7416 (1.8751 on avg)\n",
            "Epoch: [3/10][1300/1563]\tLoss: 1.5762 (1.8723 on avg)\n",
            "Epoch: [3/10][1400/1563]\tLoss: 1.7063 (1.8647 on avg)\n",
            "Epoch: [3/10][1500/1563]\tLoss: 1.7588 (1.8578 on avg)\n",
            "\n",
            "* Epoch: [3/10]\tTrain loss: 1.854\n",
            "\n",
            "Test (on val set): [3/10][   0/313]\tLoss: 1.9199 (1.9199 on avg)\tTop-1 err: 78.1250 (78.1250 on avg)\tTop-5 err: 12.5000 (12.5000 on avg)\n",
            "Test (on val set): [3/10][ 100/313]\tLoss: 1.9526 (1.9163 on avg)\tTop-1 err: 78.1250 (65.3775 on avg)\tTop-5 err: 6.2500 (13.2735 on avg)\n",
            "Test (on val set): [3/10][ 200/313]\tLoss: 2.3304 (1.9306 on avg)\tTop-1 err: 75.0000 (66.4024 on avg)\tTop-5 err: 6.2500 (13.2307 on avg)\n",
            "Test (on val set): [3/10][ 300/313]\tLoss: 1.8478 (1.9354 on avg)\tTop-1 err: 59.3750 (66.9643 on avg)\tTop-5 err: 12.5000 (13.1333 on avg)\n",
            "\n",
            "* Epoch: [3/10]\tTest loss: 1.933\tTop-1 err: 66.820\tTop-5 err: 13.110\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 32.97 3.07 \n",
            "\n",
            "Epoch: [4/10][   0/1563]\tLoss: 1.9690 (1.9690 on avg)\n",
            "Epoch: [4/10][ 100/1563]\tLoss: 1.7201 (1.7534 on avg)\n",
            "Epoch: [4/10][ 200/1563]\tLoss: 2.0308 (1.7483 on avg)\n",
            "Epoch: [4/10][ 300/1563]\tLoss: 1.4876 (1.7388 on avg)\n",
            "Epoch: [4/10][ 400/1563]\tLoss: 1.7110 (1.7275 on avg)\n",
            "Epoch: [4/10][ 500/1563]\tLoss: 1.6087 (1.7192 on avg)\n",
            "Epoch: [4/10][ 600/1563]\tLoss: 1.6575 (1.7130 on avg)\n",
            "Epoch: [4/10][ 700/1563]\tLoss: 1.5629 (1.7085 on avg)\n",
            "Epoch: [4/10][ 800/1563]\tLoss: 1.4903 (1.7043 on avg)\n",
            "Epoch: [4/10][ 900/1563]\tLoss: 1.4155 (1.6994 on avg)\n",
            "Epoch: [4/10][1000/1563]\tLoss: 1.5858 (1.6998 on avg)\n",
            "Epoch: [4/10][1100/1563]\tLoss: 1.8838 (1.6944 on avg)\n",
            "Epoch: [4/10][1200/1563]\tLoss: 1.5743 (1.6889 on avg)\n",
            "Epoch: [4/10][1300/1563]\tLoss: 1.4371 (1.6821 on avg)\n",
            "Epoch: [4/10][1400/1563]\tLoss: 1.4948 (1.6750 on avg)\n",
            "Epoch: [4/10][1500/1563]\tLoss: 1.4252 (1.6673 on avg)\n",
            "\n",
            "* Epoch: [4/10]\tTrain loss: 1.663\n",
            "\n",
            "Test (on val set): [4/10][   0/313]\tLoss: 1.6309 (1.6309 on avg)\tTop-1 err: 56.2500 (56.2500 on avg)\tTop-5 err: 3.1250 (3.1250 on avg)\n",
            "Test (on val set): [4/10][ 100/313]\tLoss: 1.0913 (1.6595 on avg)\tTop-1 err: 37.5000 (57.2710 on avg)\tTop-5 err: 6.2500 (7.7661 on avg)\n",
            "Test (on val set): [4/10][ 200/313]\tLoss: 1.8544 (1.6680 on avg)\tTop-1 err: 59.3750 (57.2761 on avg)\tTop-5 err: 12.5000 (7.8980 on avg)\n",
            "Test (on val set): [4/10][ 300/313]\tLoss: 1.8604 (1.6569 on avg)\tTop-1 err: 50.0000 (56.8314 on avg)\tTop-5 err: 12.5000 (7.9838 on avg)\n",
            "\n",
            "* Epoch: [4/10]\tTest loss: 1.657\tTop-1 err: 56.780\tTop-5 err: 7.970\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 32.97 3.07 \n",
            "\n",
            "Epoch: [5/10][   0/1563]\tLoss: 1.4550 (1.4550 on avg)\n",
            "Epoch: [5/10][ 100/1563]\tLoss: 1.6751 (1.5548 on avg)\n",
            "Epoch: [5/10][ 200/1563]\tLoss: 1.7709 (1.5608 on avg)\n",
            "Epoch: [5/10][ 300/1563]\tLoss: 1.8771 (1.5523 on avg)\n",
            "Epoch: [5/10][ 400/1563]\tLoss: 1.5031 (1.5385 on avg)\n",
            "Epoch: [5/10][ 500/1563]\tLoss: 1.3029 (1.5344 on avg)\n",
            "Epoch: [5/10][ 600/1563]\tLoss: 1.5399 (1.5267 on avg)\n",
            "Epoch: [5/10][ 700/1563]\tLoss: 1.1769 (1.5213 on avg)\n",
            "Epoch: [5/10][ 800/1563]\tLoss: 1.4197 (1.5123 on avg)\n",
            "Epoch: [5/10][ 900/1563]\tLoss: 1.5854 (1.5070 on avg)\n",
            "Epoch: [5/10][1000/1563]\tLoss: 1.2725 (1.4969 on avg)\n",
            "Epoch: [5/10][1100/1563]\tLoss: 1.5014 (1.4926 on avg)\n",
            "Epoch: [5/10][1200/1563]\tLoss: 1.2101 (1.4851 on avg)\n",
            "Epoch: [5/10][1300/1563]\tLoss: 1.5130 (1.4798 on avg)\n",
            "Epoch: [5/10][1400/1563]\tLoss: 1.5038 (1.4760 on avg)\n",
            "Epoch: [5/10][1500/1563]\tLoss: 1.1183 (1.4708 on avg)\n",
            "\n",
            "* Epoch: [5/10]\tTrain loss: 1.467\n",
            "\n",
            "Test (on val set): [5/10][   0/313]\tLoss: 2.2195 (2.2195 on avg)\tTop-1 err: 59.3750 (59.3750 on avg)\tTop-5 err: 25.0000 (25.0000 on avg)\n",
            "Test (on val set): [5/10][ 100/313]\tLoss: 1.2307 (1.5468 on avg)\tTop-1 err: 50.0000 (49.9691 on avg)\tTop-5 err: 0.0000 (6.7760 on avg)\n",
            "Test (on val set): [5/10][ 200/313]\tLoss: 1.1993 (1.5575 on avg)\tTop-1 err: 37.5000 (50.1088 on avg)\tTop-5 err: 3.1250 (6.5920 on avg)\n",
            "Test (on val set): [5/10][ 300/313]\tLoss: 1.6619 (1.5486 on avg)\tTop-1 err: 50.0000 (50.0727 on avg)\tTop-5 err: 3.1250 (6.5822 on avg)\n",
            "\n",
            "* Epoch: [5/10]\tTest loss: 1.548\tTop-1 err: 50.170\tTop-5 err: 6.520\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 32.97 3.07 \n",
            "\n",
            "Epoch: [6/10][   0/1563]\tLoss: 1.2648 (1.2648 on avg)\n",
            "Epoch: [6/10][ 100/1563]\tLoss: 1.3553 (1.3652 on avg)\n",
            "Epoch: [6/10][ 200/1563]\tLoss: 1.3603 (1.3663 on avg)\n",
            "Epoch: [6/10][ 300/1563]\tLoss: 1.5253 (1.3640 on avg)\n",
            "Epoch: [6/10][ 400/1563]\tLoss: 1.3572 (1.3570 on avg)\n",
            "Epoch: [6/10][ 500/1563]\tLoss: 1.0760 (1.3479 on avg)\n",
            "Epoch: [6/10][ 600/1563]\tLoss: 1.2425 (1.3419 on avg)\n",
            "Epoch: [6/10][ 700/1563]\tLoss: 1.5877 (1.3407 on avg)\n",
            "Epoch: [6/10][ 800/1563]\tLoss: 1.3189 (1.3354 on avg)\n",
            "Epoch: [6/10][ 900/1563]\tLoss: 1.1718 (1.3305 on avg)\n",
            "Epoch: [6/10][1000/1563]\tLoss: 1.1492 (1.3279 on avg)\n",
            "Epoch: [6/10][1100/1563]\tLoss: 1.4246 (1.3238 on avg)\n",
            "Epoch: [6/10][1200/1563]\tLoss: 1.7283 (1.3190 on avg)\n",
            "Epoch: [6/10][1300/1563]\tLoss: 1.1331 (1.3155 on avg)\n",
            "Epoch: [6/10][1400/1563]\tLoss: 1.5487 (1.3092 on avg)\n",
            "Epoch: [6/10][1500/1563]\tLoss: 1.4360 (1.3046 on avg)\n",
            "\n",
            "* Epoch: [6/10]\tTrain loss: 1.303\n",
            "\n",
            "Test (on val set): [6/10][   0/313]\tLoss: 1.5129 (1.5129 on avg)\tTop-1 err: 46.8750 (46.8750 on avg)\tTop-5 err: 0.0000 (0.0000 on avg)\n",
            "Test (on val set): [6/10][ 100/313]\tLoss: 1.4820 (1.3226 on avg)\tTop-1 err: 43.7500 (44.8020 on avg)\tTop-5 err: 9.3750 (4.4864 on avg)\n",
            "Test (on val set): [6/10][ 200/313]\tLoss: 1.3695 (1.3442 on avg)\tTop-1 err: 43.7500 (44.9160 on avg)\tTop-5 err: 6.2500 (4.3999 on avg)\n",
            "Test (on val set): [6/10][ 300/313]\tLoss: 1.5916 (1.3476 on avg)\tTop-1 err: 65.6250 (44.8401 on avg)\tTop-5 err: 9.3750 (4.4228 on avg)\n",
            "\n",
            "* Epoch: [6/10]\tTest loss: 1.351\tTop-1 err: 44.850\tTop-5 err: 4.470\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 32.97 3.07 \n",
            "\n",
            "Epoch: [7/10][   0/1563]\tLoss: 1.3526 (1.3526 on avg)\n",
            "Epoch: [7/10][ 100/1563]\tLoss: 1.0423 (1.2283 on avg)\n",
            "Epoch: [7/10][ 200/1563]\tLoss: 0.9784 (1.2278 on avg)\n",
            "Epoch: [7/10][ 300/1563]\tLoss: 1.1358 (1.2287 on avg)\n",
            "Epoch: [7/10][ 400/1563]\tLoss: 1.0601 (1.2216 on avg)\n",
            "Epoch: [7/10][ 500/1563]\tLoss: 1.2569 (1.2151 on avg)\n",
            "Epoch: [7/10][ 600/1563]\tLoss: 1.1495 (1.2173 on avg)\n",
            "Epoch: [7/10][ 700/1563]\tLoss: 1.6915 (1.2165 on avg)\n",
            "Epoch: [7/10][ 800/1563]\tLoss: 1.3937 (1.2107 on avg)\n",
            "Epoch: [7/10][ 900/1563]\tLoss: 1.4078 (1.2070 on avg)\n",
            "Epoch: [7/10][1000/1563]\tLoss: 0.9365 (1.2039 on avg)\n",
            "Epoch: [7/10][1100/1563]\tLoss: 0.8385 (1.1999 on avg)\n",
            "Epoch: [7/10][1200/1563]\tLoss: 0.8363 (1.1956 on avg)\n",
            "Epoch: [7/10][1300/1563]\tLoss: 1.1023 (1.1909 on avg)\n",
            "Epoch: [7/10][1400/1563]\tLoss: 1.1996 (1.1883 on avg)\n",
            "Epoch: [7/10][1500/1563]\tLoss: 1.2095 (1.1862 on avg)\n",
            "\n",
            "* Epoch: [7/10]\tTrain loss: 1.183\n",
            "\n",
            "Test (on val set): [7/10][   0/313]\tLoss: 1.4974 (1.4974 on avg)\tTop-1 err: 46.8750 (46.8750 on avg)\tTop-5 err: 6.2500 (6.2500 on avg)\n",
            "Test (on val set): [7/10][ 100/313]\tLoss: 1.8073 (1.3651 on avg)\tTop-1 err: 50.0000 (40.9344 on avg)\tTop-5 err: 3.1250 (4.2698 on avg)\n",
            "Test (on val set): [7/10][ 200/313]\tLoss: 1.3385 (1.3845 on avg)\tTop-1 err: 50.0000 (41.6045 on avg)\tTop-5 err: 6.2500 (4.2289 on avg)\n",
            "Test (on val set): [7/10][ 300/313]\tLoss: 0.8344 (1.3750 on avg)\tTop-1 err: 28.1250 (41.6840 on avg)\tTop-5 err: 6.2500 (4.1840 on avg)\n",
            "\n",
            "* Epoch: [7/10]\tTest loss: 1.375\tTop-1 err: 41.630\tTop-5 err: 4.250\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 32.97 3.07 \n",
            "\n",
            "Epoch: [8/10][   0/1563]\tLoss: 0.9186 (0.9186 on avg)\n",
            "Epoch: [8/10][ 100/1563]\tLoss: 1.1152 (1.1243 on avg)\n",
            "Epoch: [8/10][ 200/1563]\tLoss: 1.2641 (1.1342 on avg)\n",
            "Epoch: [8/10][ 300/1563]\tLoss: 1.3618 (1.1231 on avg)\n",
            "Epoch: [8/10][ 400/1563]\tLoss: 1.1591 (1.1182 on avg)\n",
            "Epoch: [8/10][ 500/1563]\tLoss: 0.7977 (1.1166 on avg)\n",
            "Epoch: [8/10][ 600/1563]\tLoss: 1.3144 (1.1139 on avg)\n",
            "Epoch: [8/10][ 700/1563]\tLoss: 0.7489 (1.1142 on avg)\n",
            "Epoch: [8/10][ 800/1563]\tLoss: 1.0480 (1.1125 on avg)\n",
            "Epoch: [8/10][ 900/1563]\tLoss: 1.3131 (1.1098 on avg)\n",
            "Epoch: [8/10][1000/1563]\tLoss: 1.2410 (1.1064 on avg)\n",
            "Epoch: [8/10][1100/1563]\tLoss: 1.1206 (1.1019 on avg)\n",
            "Epoch: [8/10][1200/1563]\tLoss: 0.8771 (1.0998 on avg)\n",
            "Epoch: [8/10][1300/1563]\tLoss: 1.1352 (1.0979 on avg)\n",
            "Epoch: [8/10][1400/1563]\tLoss: 0.9347 (1.0937 on avg)\n",
            "Epoch: [8/10][1500/1563]\tLoss: 1.2038 (1.0897 on avg)\n",
            "\n",
            "* Epoch: [8/10]\tTrain loss: 1.086\n",
            "\n",
            "Test (on val set): [8/10][   0/313]\tLoss: 1.0250 (1.0250 on avg)\tTop-1 err: 31.2500 (31.2500 on avg)\tTop-5 err: 0.0000 (0.0000 on avg)\n",
            "Test (on val set): [8/10][ 100/313]\tLoss: 0.9511 (1.1503 on avg)\tTop-1 err: 31.2500 (37.0978 on avg)\tTop-5 err: 3.1250 (3.0941 on avg)\n",
            "Test (on val set): [8/10][ 200/313]\tLoss: 0.9759 (1.1408 on avg)\tTop-1 err: 31.2500 (36.4739 on avg)\tTop-5 err: 3.1250 (3.2494 on avg)\n",
            "Test (on val set): [8/10][ 300/313]\tLoss: 0.9115 (1.1456 on avg)\tTop-1 err: 25.0000 (36.4306 on avg)\tTop-5 err: 6.2500 (3.1561 on avg)\n",
            "\n",
            "* Epoch: [8/10]\tTest loss: 1.146\tTop-1 err: 36.550\tTop-5 err: 3.120\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 32.97 3.07 \n",
            "\n",
            "Epoch: [9/10][   0/1563]\tLoss: 0.9885 (0.9885 on avg)\n",
            "Epoch: [9/10][ 100/1563]\tLoss: 1.5339 (1.0254 on avg)\n",
            "Epoch: [9/10][ 200/1563]\tLoss: 0.9318 (1.0119 on avg)\n",
            "Epoch: [9/10][ 300/1563]\tLoss: 1.0936 (1.0068 on avg)\n",
            "Epoch: [9/10][ 400/1563]\tLoss: 0.8306 (1.0077 on avg)\n",
            "Epoch: [9/10][ 500/1563]\tLoss: 0.9975 (1.0052 on avg)\n",
            "Epoch: [9/10][ 600/1563]\tLoss: 1.1751 (1.0065 on avg)\n",
            "Epoch: [9/10][ 700/1563]\tLoss: 0.9179 (1.0046 on avg)\n",
            "Epoch: [9/10][ 800/1563]\tLoss: 0.7194 (1.0024 on avg)\n",
            "Epoch: [9/10][ 900/1563]\tLoss: 1.0556 (1.0050 on avg)\n",
            "Epoch: [9/10][1000/1563]\tLoss: 0.8905 (1.0039 on avg)\n",
            "Epoch: [9/10][1100/1563]\tLoss: 0.9614 (1.0011 on avg)\n",
            "Epoch: [9/10][1200/1563]\tLoss: 1.0743 (0.9973 on avg)\n",
            "Epoch: [9/10][1300/1563]\tLoss: 0.9081 (0.9955 on avg)\n",
            "Epoch: [9/10][1400/1563]\tLoss: 0.7777 (0.9935 on avg)\n",
            "Epoch: [9/10][1500/1563]\tLoss: 0.6693 (0.9910 on avg)\n",
            "\n",
            "* Epoch: [9/10]\tTrain loss: 0.991\n",
            "\n",
            "Test (on val set): [9/10][   0/313]\tLoss: 0.7720 (0.7720 on avg)\tTop-1 err: 21.8750 (21.8750 on avg)\tTop-5 err: 3.1250 (3.1250 on avg)\n",
            "Test (on val set): [9/10][ 100/313]\tLoss: 0.8583 (1.0739 on avg)\tTop-1 err: 31.2500 (33.2921 on avg)\tTop-5 err: 0.0000 (2.8775 on avg)\n",
            "Test (on val set): [9/10][ 200/313]\tLoss: 1.1614 (1.0441 on avg)\tTop-1 err: 31.2500 (33.0379 on avg)\tTop-5 err: 3.1250 (2.7052 on avg)\n",
            "Test (on val set): [9/10][ 300/313]\tLoss: 1.1781 (1.0637 on avg)\tTop-1 err: 31.2500 (33.5029 on avg)\tTop-5 err: 6.2500 (2.7616 on avg)\n",
            "\n",
            "* Epoch: [9/10]\tTest loss: 1.060\tTop-1 err: 33.510\tTop-5 err: 2.720\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 32.97 3.07 \n",
            "\n",
            "Epoch: [10/10][   0/1563]\tLoss: 1.0102 (1.0102 on avg)\n",
            "Epoch: [10/10][ 100/1563]\tLoss: 1.3093 (0.9263 on avg)\n",
            "Epoch: [10/10][ 200/1563]\tLoss: 0.6234 (0.9098 on avg)\n",
            "Epoch: [10/10][ 300/1563]\tLoss: 1.0352 (0.9131 on avg)\n",
            "Epoch: [10/10][ 400/1563]\tLoss: 0.8735 (0.9081 on avg)\n",
            "Epoch: [10/10][ 500/1563]\tLoss: 1.0285 (0.9072 on avg)\n",
            "Epoch: [10/10][ 600/1563]\tLoss: 1.0429 (0.9080 on avg)\n",
            "Epoch: [10/10][ 700/1563]\tLoss: 0.7028 (0.9127 on avg)\n",
            "Epoch: [10/10][ 800/1563]\tLoss: 1.0667 (0.9135 on avg)\n",
            "Epoch: [10/10][ 900/1563]\tLoss: 0.7471 (0.9110 on avg)\n",
            "Epoch: [10/10][1000/1563]\tLoss: 0.8655 (0.9096 on avg)\n",
            "Epoch: [10/10][1100/1563]\tLoss: 1.1026 (0.9076 on avg)\n",
            "Epoch: [10/10][1200/1563]\tLoss: 0.8555 (0.9035 on avg)\n",
            "Epoch: [10/10][1300/1563]\tLoss: 1.1260 (0.9040 on avg)\n",
            "Epoch: [10/10][1400/1563]\tLoss: 0.8888 (0.9017 on avg)\n",
            "Epoch: [10/10][1500/1563]\tLoss: 1.0594 (0.9028 on avg)\n",
            "\n",
            "* Epoch: [10/10]\tTrain loss: 0.902\n",
            "\n",
            "Test (on val set): [10/10][   0/313]\tLoss: 1.0142 (1.0142 on avg)\tTop-1 err: 28.1250 (28.1250 on avg)\tTop-5 err: 0.0000 (0.0000 on avg)\n",
            "Test (on val set): [10/10][ 100/313]\tLoss: 1.0292 (1.0416 on avg)\tTop-1 err: 37.5000 (31.7760 on avg)\tTop-5 err: 0.0000 (2.5990 on avg)\n",
            "Test (on val set): [10/10][ 200/313]\tLoss: 0.7816 (1.0282 on avg)\tTop-1 err: 25.0000 (31.6542 on avg)\tTop-5 err: 0.0000 (2.5498 on avg)\n",
            "Test (on val set): [10/10][ 300/313]\tLoss: 0.6121 (1.0202 on avg)\tTop-1 err: 18.7500 (31.3123 on avg)\tTop-5 err: 0.0000 (2.4605 on avg)\n",
            "\n",
            "* Epoch: [10/10]\tTest loss: 1.023\tTop-1 err: 31.320\tTop-5 err: 2.470\n",
            "\n",
            "Current best error rate (top-1 and top-5 error): 31.32 2.47 \n",
            "\n",
            "Best error rate (top-1 and top-5 error): 31.32 2.47\n",
            "Learning Rate: 0.001\n"
          ]
        }
      ],
      "source": [
        "# Run the main function.\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # select a model to train here (CIFAR10Net or MNISTNet)\n",
        "    model = CIFAR10Net()\n",
        "\n",
        "    # move to GPU\n",
        "    model.to(device)\n",
        "\n",
        "    # select number of epochs\n",
        "    epochs = 10\n",
        "\n",
        "    # get criterion and optimizer\n",
        "    criterion, optimizer = get_crit_and_opt(model)\n",
        "\n",
        "    # epoch loop\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        # train for one epoch\n",
        "        train_loss = train(\n",
        "          train_loader,\n",
        "          model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          epoch,\n",
        "          epochs\n",
        "        )\n",
        "\n",
        "        # evaluate on validation set\n",
        "        err1, err5, val_loss = validate(\n",
        "          val_loader,\n",
        "          model,\n",
        "          criterion,\n",
        "          epoch,\n",
        "          epochs\n",
        "        )\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = err1 <= best_err1\n",
        "        best_err1 = min(err1, best_err1)\n",
        "        if is_best:\n",
        "            best_err5 = err5\n",
        "\n",
        "        print('Current best error rate (top-1 and top-5 error):', best_err1, best_err5, '\\n')\n",
        "    print('Best error rate (top-1 and top-5 error):', best_err1, best_err5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLaHPLiFmI54",
        "outputId": "2bf022d5-7962-4b7f-aed6-37314fe0f2df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1/10][   0/1563]\tLoss: 2.3077 (2.3077 on avg)\n",
            "Epoch: [1/10][ 100/1563]\tLoss: 2.3020 (2.3029 on avg)\n",
            "Epoch: [1/10][ 200/1563]\tLoss: 2.3101 (2.3026 on avg)\n",
            "Epoch: [1/10][ 300/1563]\tLoss: 2.2959 (2.3027 on avg)\n",
            "Epoch: [1/10][ 400/1563]\tLoss: 2.3103 (2.3027 on avg)\n",
            "Epoch: [1/10][ 500/1563]\tLoss: 2.3017 (2.3027 on avg)\n",
            "Epoch: [1/10][ 600/1563]\tLoss: 2.3058 (2.3027 on avg)\n",
            "Epoch: [1/10][ 700/1563]\tLoss: 2.3009 (2.3027 on avg)\n",
            "Epoch: [1/10][ 800/1563]\tLoss: 2.3003 (2.3027 on avg)\n",
            "Epoch: [1/10][ 900/1563]\tLoss: 2.3035 (2.3027 on avg)\n",
            "Epoch: [1/10][1000/1563]\tLoss: 2.3025 (2.3026 on avg)\n",
            "Epoch: [1/10][1100/1563]\tLoss: 2.3012 (2.3026 on avg)\n",
            "Epoch: [1/10][1200/1563]\tLoss: 2.3015 (2.3026 on avg)\n",
            "Epoch: [1/10][1300/1563]\tLoss: 2.3016 (2.3025 on avg)\n",
            "Epoch: [1/10][1400/1563]\tLoss: 2.3022 (2.3025 on avg)\n",
            "Epoch: [1/10][1500/1563]\tLoss: 2.3009 (2.3024 on avg)\n",
            "\n",
            "* Epoch: [1/10]\tTrain loss: 2.302\n",
            "\n",
            "Test (on val set): [1/10][   0/313]\tLoss: 2.3018 (2.3018 on avg)\tTop-1 err: 93.7500 (93.7500 on avg)\tTop-5 err: 56.2500 (56.2500 on avg)\n",
            "Test (on val set): [1/10][ 100/313]\tLoss: 2.2993 (2.3003 on avg)\tTop-1 err: 90.6250 (90.5322 on avg)\tTop-5 err: 43.7500 (44.5235 on avg)\n",
            "Test (on val set): [1/10][ 200/313]\tLoss: 2.3012 (2.3000 on avg)\tTop-1 err: 96.8750 (90.2674 on avg)\tTop-5 err: 46.8750 (44.6362 on avg)\n",
            "Test (on val set): [1/10][ 300/313]\tLoss: 2.3025 (2.2999 on avg)\tTop-1 err: 90.6250 (90.0125 on avg)\tTop-5 err: 53.1250 (44.7467 on avg)\n",
            "\n",
            "* Epoch: [1/10]\tTest loss: 2.300\tTop-1 err: 90.010\tTop-5 err: 44.670\n",
            "\n",
            "Dropout: 0.25, Noise: 0.1529, Epoch: 1, Train Loss: 2.30236993270874, Val Loss: 2.299930059814453, Top-1 Error: 90.01, Top-5 Error: 44.67\n",
            "Epoch: [2/10][   0/1563]\tLoss: 2.3049 (2.3049 on avg)\n",
            "Epoch: [2/10][ 100/1563]\tLoss: 2.3014 (2.3013 on avg)\n",
            "Epoch: [2/10][ 200/1563]\tLoss: 2.2966 (2.3013 on avg)\n",
            "Epoch: [2/10][ 300/1563]\tLoss: 2.2978 (2.3011 on avg)\n",
            "Epoch: [2/10][ 400/1563]\tLoss: 2.3001 (2.3009 on avg)\n",
            "Epoch: [2/10][ 500/1563]\tLoss: 2.2972 (2.3006 on avg)\n",
            "Epoch: [2/10][ 600/1563]\tLoss: 2.2940 (2.3002 on avg)\n",
            "Epoch: [2/10][ 700/1563]\tLoss: 2.2950 (2.2997 on avg)\n",
            "Epoch: [2/10][ 800/1563]\tLoss: 2.3077 (2.2992 on avg)\n",
            "Epoch: [2/10][ 900/1563]\tLoss: 2.2949 (2.2982 on avg)\n",
            "Epoch: [2/10][1000/1563]\tLoss: 2.2304 (2.2963 on avg)\n",
            "Epoch: [2/10][1100/1563]\tLoss: 2.1757 (2.2939 on avg)\n",
            "Epoch: [2/10][1200/1563]\tLoss: 2.2878 (2.2909 on avg)\n",
            "Epoch: [2/10][1300/1563]\tLoss: 2.2514 (2.2857 on avg)\n",
            "Epoch: [2/10][1400/1563]\tLoss: 2.0431 (2.2799 on avg)\n",
            "Epoch: [2/10][1500/1563]\tLoss: 2.2039 (2.2721 on avg)\n",
            "\n",
            "* Epoch: [2/10]\tTrain loss: 2.266\n",
            "\n",
            "Test (on val set): [2/10][   0/313]\tLoss: 1.9845 (1.9845 on avg)\tTop-1 err: 78.1250 (78.1250 on avg)\tTop-5 err: 21.8750 (21.8750 on avg)\n",
            "Test (on val set): [2/10][ 100/313]\tLoss: 2.3388 (2.0804 on avg)\tTop-1 err: 90.6250 (78.0012 on avg)\tTop-5 err: 21.8750 (23.2673 on avg)\n",
            "Test (on val set): [2/10][ 200/313]\tLoss: 2.0276 (2.0754 on avg)\tTop-1 err: 78.1250 (77.4720 on avg)\tTop-5 err: 28.1250 (22.9011 on avg)\n",
            "Test (on val set): [2/10][ 300/313]\tLoss: 1.8952 (2.0822 on avg)\tTop-1 err: 78.1250 (77.3463 on avg)\tTop-5 err: 15.6250 (22.7990 on avg)\n",
            "\n",
            "* Epoch: [2/10]\tTest loss: 2.083\tTop-1 err: 77.390\tTop-5 err: 22.930\n",
            "\n",
            "Dropout: 0.25, Noise: 0.1529, Epoch: 2, Train Loss: 2.26579344871521, Val Loss: 2.0827002841949462, Top-1 Error: 77.39, Top-5 Error: 22.93\n",
            "Epoch: [3/10][   0/1563]\tLoss: 2.1759 (2.1759 on avg)\n",
            "Epoch: [3/10][ 100/1563]\tLoss: 2.1067 (2.0773 on avg)\n",
            "Epoch: [3/10][ 200/1563]\tLoss: 1.8677 (2.0603 on avg)\n",
            "Epoch: [3/10][ 300/1563]\tLoss: 2.0864 (2.0383 on avg)\n",
            "Epoch: [3/10][ 400/1563]\tLoss: 2.0510 (2.0207 on avg)\n",
            "Epoch: [3/10][ 500/1563]\tLoss: 1.8196 (2.0026 on avg)\n",
            "Epoch: [3/10][ 600/1563]\tLoss: 2.1880 (1.9918 on avg)\n",
            "Epoch: [3/10][ 700/1563]\tLoss: 1.9584 (1.9817 on avg)\n",
            "Epoch: [3/10][ 800/1563]\tLoss: 1.6932 (1.9688 on avg)\n",
            "Epoch: [3/10][ 900/1563]\tLoss: 2.0373 (1.9588 on avg)\n",
            "Epoch: [3/10][1000/1563]\tLoss: 1.7471 (1.9476 on avg)\n",
            "Epoch: [3/10][1100/1563]\tLoss: 1.6252 (1.9361 on avg)\n",
            "Epoch: [3/10][1200/1563]\tLoss: 1.8452 (1.9266 on avg)\n",
            "Epoch: [3/10][1300/1563]\tLoss: 1.8118 (1.9170 on avg)\n",
            "Epoch: [3/10][1400/1563]\tLoss: 1.9603 (1.9088 on avg)\n",
            "Epoch: [3/10][1500/1563]\tLoss: 1.8229 (1.8985 on avg)\n",
            "\n",
            "* Epoch: [3/10]\tTrain loss: 1.893\n",
            "\n",
            "Test (on val set): [3/10][   0/313]\tLoss: 1.5430 (1.5430 on avg)\tTop-1 err: 62.5000 (62.5000 on avg)\tTop-5 err: 9.3750 (9.3750 on avg)\n",
            "Test (on val set): [3/10][ 100/313]\tLoss: 2.1275 (1.9590 on avg)\tTop-1 err: 78.1250 (66.7079 on avg)\tTop-5 err: 12.5000 (14.2327 on avg)\n",
            "Test (on val set): [3/10][ 200/313]\tLoss: 2.1572 (1.9809 on avg)\tTop-1 err: 75.0000 (67.0709 on avg)\tTop-5 err: 21.8750 (14.6766 on avg)\n",
            "Test (on val set): [3/10][ 300/313]\tLoss: 2.4198 (1.9676 on avg)\tTop-1 err: 81.2500 (67.0993 on avg)\tTop-5 err: 18.7500 (14.2650 on avg)\n",
            "\n",
            "* Epoch: [3/10]\tTest loss: 1.967\tTop-1 err: 67.080\tTop-5 err: 14.230\n",
            "\n",
            "Dropout: 0.25, Noise: 0.1529, Epoch: 3, Train Loss: 1.892984094696045, Val Loss: 1.9670168825149537, Top-1 Error: 67.08, Top-5 Error: 14.23\n",
            "Epoch: [4/10][   0/1563]\tLoss: 1.7713 (1.7713 on avg)\n",
            "Epoch: [4/10][ 100/1563]\tLoss: 1.7003 (1.7302 on avg)\n",
            "Epoch: [4/10][ 200/1563]\tLoss: 1.7937 (1.7368 on avg)\n",
            "Epoch: [4/10][ 300/1563]\tLoss: 1.8101 (1.7384 on avg)\n",
            "Epoch: [4/10][ 400/1563]\tLoss: 1.7085 (1.7279 on avg)\n",
            "Epoch: [4/10][ 500/1563]\tLoss: 1.6732 (1.7125 on avg)\n",
            "Epoch: [4/10][ 600/1563]\tLoss: 1.8103 (1.7051 on avg)\n",
            "Epoch: [4/10][ 700/1563]\tLoss: 1.8031 (1.6997 on avg)\n",
            "Epoch: [4/10][ 800/1563]\tLoss: 1.7714 (1.6928 on avg)\n",
            "Epoch: [4/10][ 900/1563]\tLoss: 1.4975 (1.6861 on avg)\n",
            "Epoch: [4/10][1000/1563]\tLoss: 1.6409 (1.6768 on avg)\n",
            "Epoch: [4/10][1100/1563]\tLoss: 2.0335 (1.6683 on avg)\n",
            "Epoch: [4/10][1200/1563]\tLoss: 1.5027 (1.6615 on avg)\n",
            "Epoch: [4/10][1300/1563]\tLoss: 1.4000 (1.6553 on avg)\n",
            "Epoch: [4/10][1400/1563]\tLoss: 1.6812 (1.6452 on avg)\n",
            "Epoch: [4/10][1500/1563]\tLoss: 1.2507 (1.6383 on avg)\n",
            "\n",
            "* Epoch: [4/10]\tTrain loss: 1.634\n",
            "\n",
            "Test (on val set): [4/10][   0/313]\tLoss: 1.5068 (1.5068 on avg)\tTop-1 err: 53.1250 (53.1250 on avg)\tTop-5 err: 6.2500 (6.2500 on avg)\n",
            "Test (on val set): [4/10][ 100/313]\tLoss: 1.7080 (1.5537 on avg)\tTop-1 err: 59.3750 (55.7859 on avg)\tTop-5 err: 6.2500 (6.9926 on avg)\n",
            "Test (on val set): [4/10][ 200/313]\tLoss: 2.0964 (1.5351 on avg)\tTop-1 err: 65.6250 (55.2550 on avg)\tTop-5 err: 18.7500 (7.0740 on avg)\n",
            "Test (on val set): [4/10][ 300/313]\tLoss: 2.1880 (1.5439 on avg)\tTop-1 err: 56.2500 (55.2533 on avg)\tTop-5 err: 12.5000 (7.2986 on avg)\n",
            "\n",
            "* Epoch: [4/10]\tTest loss: 1.540\tTop-1 err: 55.190\tTop-5 err: 7.200\n",
            "\n",
            "Dropout: 0.25, Noise: 0.1529, Epoch: 4, Train Loss: 1.6341558232879638, Val Loss: 1.5403841005325318, Top-1 Error: 55.19, Top-5 Error: 7.2\n",
            "Epoch: [5/10][   0/1563]\tLoss: 1.4650 (1.4650 on avg)\n",
            "Epoch: [5/10][ 100/1563]\tLoss: 1.6620 (1.5575 on avg)\n",
            "Epoch: [5/10][ 200/1563]\tLoss: 1.4413 (1.5300 on avg)\n",
            "Epoch: [5/10][ 300/1563]\tLoss: 1.4308 (1.5120 on avg)\n",
            "Epoch: [5/10][ 400/1563]\tLoss: 1.4940 (1.5070 on avg)\n",
            "Epoch: [5/10][ 500/1563]\tLoss: 1.4351 (1.5020 on avg)\n",
            "Epoch: [5/10][ 600/1563]\tLoss: 1.3180 (1.4922 on avg)\n",
            "Epoch: [5/10][ 700/1563]\tLoss: 1.2418 (1.4844 on avg)\n",
            "Epoch: [5/10][ 800/1563]\tLoss: 1.4041 (1.4758 on avg)\n",
            "Epoch: [5/10][ 900/1563]\tLoss: 1.2344 (1.4668 on avg)\n",
            "Epoch: [5/10][1000/1563]\tLoss: 1.5551 (1.4629 on avg)\n",
            "Epoch: [5/10][1100/1563]\tLoss: 1.2853 (1.4589 on avg)\n",
            "Epoch: [5/10][1200/1563]\tLoss: 1.3964 (1.4523 on avg)\n",
            "Epoch: [5/10][1300/1563]\tLoss: 1.3564 (1.4483 on avg)\n",
            "Epoch: [5/10][1400/1563]\tLoss: 1.6415 (1.4422 on avg)\n",
            "Epoch: [5/10][1500/1563]\tLoss: 1.4597 (1.4363 on avg)\n",
            "\n",
            "* Epoch: [5/10]\tTrain loss: 1.433\n",
            "\n",
            "Test (on val set): [5/10][   0/313]\tLoss: 1.5181 (1.5181 on avg)\tTop-1 err: 40.6250 (40.6250 on avg)\tTop-5 err: 6.2500 (6.2500 on avg)\n",
            "Test (on val set): [5/10][ 100/313]\tLoss: 1.0757 (1.4439 on avg)\tTop-1 err: 43.7500 (49.0099 on avg)\tTop-5 err: 3.1250 (5.2599 on avg)\n",
            "Test (on val set): [5/10][ 200/313]\tLoss: 1.4503 (1.4479 on avg)\tTop-1 err: 40.6250 (48.7407 on avg)\tTop-5 err: 6.2500 (5.6903 on avg)\n",
            "Test (on val set): [5/10][ 300/313]\tLoss: 1.3289 (1.4372 on avg)\tTop-1 err: 50.0000 (48.6400 on avg)\tTop-5 err: 3.1250 (5.7413 on avg)\n",
            "\n",
            "* Epoch: [5/10]\tTest loss: 1.435\tTop-1 err: 48.640\tTop-5 err: 5.720\n",
            "\n",
            "Dropout: 0.25, Noise: 0.1529, Epoch: 5, Train Loss: 1.432807759552002, Val Loss: 1.4348611698150635, Top-1 Error: 48.64, Top-5 Error: 5.72\n",
            "Epoch: [6/10][   0/1563]\tLoss: 1.4634 (1.4634 on avg)\n",
            "Epoch: [6/10][ 100/1563]\tLoss: 1.3354 (1.3196 on avg)\n",
            "Epoch: [6/10][ 200/1563]\tLoss: 1.4241 (1.3278 on avg)\n",
            "Epoch: [6/10][ 300/1563]\tLoss: 1.3949 (1.3177 on avg)\n",
            "Epoch: [6/10][ 400/1563]\tLoss: 1.3987 (1.3039 on avg)\n",
            "Epoch: [6/10][ 500/1563]\tLoss: 1.6282 (1.2990 on avg)\n",
            "Epoch: [6/10][ 600/1563]\tLoss: 1.5004 (1.2941 on avg)\n",
            "Epoch: [6/10][ 700/1563]\tLoss: 1.4716 (1.2915 on avg)\n",
            "Epoch: [6/10][ 800/1563]\tLoss: 1.0399 (1.2842 on avg)\n",
            "Epoch: [6/10][ 900/1563]\tLoss: 0.9590 (1.2794 on avg)\n",
            "Epoch: [6/10][1000/1563]\tLoss: 1.1944 (1.2769 on avg)\n",
            "Epoch: [6/10][1100/1563]\tLoss: 1.1313 (1.2718 on avg)\n",
            "Epoch: [6/10][1200/1563]\tLoss: 1.3639 (1.2676 on avg)\n",
            "Epoch: [6/10][1300/1563]\tLoss: 1.0382 (1.2608 on avg)\n",
            "Epoch: [6/10][1400/1563]\tLoss: 1.2465 (1.2562 on avg)\n",
            "Epoch: [6/10][1500/1563]\tLoss: 1.4998 (1.2518 on avg)\n",
            "\n",
            "* Epoch: [6/10]\tTrain loss: 1.249\n",
            "\n",
            "Test (on val set): [6/10][   0/313]\tLoss: 1.2148 (1.2148 on avg)\tTop-1 err: 34.3750 (34.3750 on avg)\tTop-5 err: 3.1250 (3.1250 on avg)\n",
            "Test (on val set): [6/10][ 100/313]\tLoss: 1.1263 (1.3814 on avg)\tTop-1 err: 28.1250 (42.2339 on avg)\tTop-5 err: 3.1250 (4.2079 on avg)\n",
            "Test (on val set): [6/10][ 200/313]\tLoss: 1.4071 (1.3726 on avg)\tTop-1 err: 40.6250 (42.8327 on avg)\tTop-5 err: 6.2500 (4.0578 on avg)\n",
            "Test (on val set): [6/10][ 300/313]\tLoss: 1.5667 (1.3594 on avg)\tTop-1 err: 56.2500 (42.6391 on avg)\tTop-5 err: 12.5000 (4.0386 on avg)\n",
            "\n",
            "* Epoch: [6/10]\tTest loss: 1.362\tTop-1 err: 42.690\tTop-5 err: 4.130\n",
            "\n",
            "Dropout: 0.25, Noise: 0.1529, Epoch: 6, Train Loss: 1.248701858215332, Val Loss: 1.3617257406234742, Top-1 Error: 42.69, Top-5 Error: 4.13\n",
            "Epoch: [7/10][   0/1563]\tLoss: 0.7293 (0.7293 on avg)\n",
            "Epoch: [7/10][ 100/1563]\tLoss: 1.3050 (1.1529 on avg)\n",
            "Epoch: [7/10][ 200/1563]\tLoss: 1.4869 (1.1553 on avg)\n",
            "Epoch: [7/10][ 300/1563]\tLoss: 1.3621 (1.1494 on avg)\n",
            "Epoch: [7/10][ 400/1563]\tLoss: 1.0738 (1.1483 on avg)\n",
            "Epoch: [7/10][ 500/1563]\tLoss: 1.1368 (1.1501 on avg)\n",
            "Epoch: [7/10][ 600/1563]\tLoss: 1.9271 (1.1436 on avg)\n",
            "Epoch: [7/10][ 700/1563]\tLoss: 1.0539 (1.1410 on avg)\n",
            "Epoch: [7/10][ 800/1563]\tLoss: 1.0169 (1.1357 on avg)\n",
            "Epoch: [7/10][ 900/1563]\tLoss: 1.1798 (1.1324 on avg)\n",
            "Epoch: [7/10][1000/1563]\tLoss: 1.4437 (1.1339 on avg)\n",
            "Epoch: [7/10][1100/1563]\tLoss: 1.1412 (1.1290 on avg)\n",
            "Epoch: [7/10][1200/1563]\tLoss: 1.2499 (1.1246 on avg)\n",
            "Epoch: [7/10][1300/1563]\tLoss: 0.9403 (1.1241 on avg)\n",
            "Epoch: [7/10][1400/1563]\tLoss: 1.0773 (1.1224 on avg)\n",
            "Epoch: [7/10][1500/1563]\tLoss: 1.2107 (1.1165 on avg)\n",
            "\n",
            "* Epoch: [7/10]\tTrain loss: 1.113\n",
            "\n",
            "Test (on val set): [7/10][   0/313]\tLoss: 1.1285 (1.1285 on avg)\tTop-1 err: 40.6250 (40.6250 on avg)\tTop-5 err: 3.1250 (3.1250 on avg)\n",
            "Test (on val set): [7/10][ 100/313]\tLoss: 1.1261 (1.1429 on avg)\tTop-1 err: 31.2500 (36.7884 on avg)\tTop-5 err: 9.3750 (2.6609 on avg)\n",
            "Test (on val set): [7/10][ 200/313]\tLoss: 1.0699 (1.1390 on avg)\tTop-1 err: 34.3750 (36.9714 on avg)\tTop-5 err: 3.1250 (2.8296 on avg)\n",
            "Test (on val set): [7/10][ 300/313]\tLoss: 1.1295 (1.1465 on avg)\tTop-1 err: 31.2500 (37.1574 on avg)\tTop-5 err: 3.1250 (3.0731 on avg)\n",
            "\n",
            "* Epoch: [7/10]\tTest loss: 1.144\tTop-1 err: 37.130\tTop-5 err: 3.020\n",
            "\n",
            "Dropout: 0.25, Noise: 0.1529, Epoch: 7, Train Loss: 1.1133177801132201, Val Loss: 1.1439071324348449, Top-1 Error: 37.13, Top-5 Error: 3.02\n",
            "Epoch: [8/10][   0/1563]\tLoss: 1.1062 (1.1062 on avg)\n",
            "Epoch: [8/10][ 100/1563]\tLoss: 1.3531 (1.0840 on avg)\n",
            "Epoch: [8/10][ 200/1563]\tLoss: 0.9783 (1.0513 on avg)\n",
            "Epoch: [8/10][ 300/1563]\tLoss: 0.8361 (1.0472 on avg)\n",
            "Epoch: [8/10][ 400/1563]\tLoss: 0.9466 (1.0535 on avg)\n",
            "Epoch: [8/10][ 500/1563]\tLoss: 0.6682 (1.0430 on avg)\n",
            "Epoch: [8/10][ 600/1563]\tLoss: 0.8775 (1.0337 on avg)\n",
            "Epoch: [8/10][ 700/1563]\tLoss: 0.9282 (1.0323 on avg)\n",
            "Epoch: [8/10][ 800/1563]\tLoss: 0.8991 (1.0253 on avg)\n",
            "Epoch: [8/10][ 900/1563]\tLoss: 0.9681 (1.0206 on avg)\n",
            "Epoch: [8/10][1000/1563]\tLoss: 0.8169 (1.0201 on avg)\n",
            "Epoch: [8/10][1100/1563]\tLoss: 1.0430 (1.0133 on avg)\n",
            "Epoch: [8/10][1200/1563]\tLoss: 1.0601 (1.0093 on avg)\n",
            "Epoch: [8/10][1300/1563]\tLoss: 1.0049 (1.0096 on avg)\n",
            "Epoch: [8/10][1400/1563]\tLoss: 1.1305 (1.0056 on avg)\n",
            "Epoch: [8/10][1500/1563]\tLoss: 1.0455 (1.0028 on avg)\n",
            "\n",
            "* Epoch: [8/10]\tTrain loss: 1.002\n",
            "\n",
            "Test (on val set): [8/10][   0/313]\tLoss: 1.5217 (1.5217 on avg)\tTop-1 err: 50.0000 (50.0000 on avg)\tTop-5 err: 0.0000 (0.0000 on avg)\n",
            "Test (on val set): [8/10][ 100/313]\tLoss: 0.8611 (1.0040 on avg)\tTop-1 err: 31.2500 (33.6634 on avg)\tTop-5 err: 0.0000 (2.6609 on avg)\n",
            "Test (on val set): [8/10][ 200/313]\tLoss: 0.8796 (1.0033 on avg)\tTop-1 err: 28.1250 (33.1934 on avg)\tTop-5 err: 0.0000 (2.6430 on avg)\n",
            "Test (on val set): [8/10][ 300/313]\tLoss: 1.6284 (1.0084 on avg)\tTop-1 err: 43.7500 (33.3368 on avg)\tTop-5 err: 6.2500 (2.5748 on avg)\n",
            "\n",
            "* Epoch: [8/10]\tTest loss: 1.012\tTop-1 err: 33.590\tTop-5 err: 2.540\n",
            "\n",
            "Dropout: 0.25, Noise: 0.1529, Epoch: 8, Train Loss: 1.0019809366607666, Val Loss: 1.011962204837799, Top-1 Error: 33.59, Top-5 Error: 2.54\n",
            "Epoch: [9/10][   0/1563]\tLoss: 0.9653 (0.9653 on avg)\n",
            "Epoch: [9/10][ 100/1563]\tLoss: 0.7441 (0.8893 on avg)\n",
            "Epoch: [9/10][ 200/1563]\tLoss: 0.7854 (0.9140 on avg)\n",
            "Epoch: [9/10][ 300/1563]\tLoss: 1.0328 (0.9119 on avg)\n",
            "Epoch: [9/10][ 400/1563]\tLoss: 1.0095 (0.9115 on avg)\n",
            "Epoch: [9/10][ 500/1563]\tLoss: 0.7876 (0.9154 on avg)\n",
            "Epoch: [9/10][ 600/1563]\tLoss: 0.8849 (0.9173 on avg)\n",
            "Epoch: [9/10][ 700/1563]\tLoss: 1.0120 (0.9196 on avg)\n",
            "Epoch: [9/10][ 800/1563]\tLoss: 1.0908 (0.9180 on avg)\n",
            "Epoch: [9/10][ 900/1563]\tLoss: 0.7551 (0.9121 on avg)\n",
            "Epoch: [9/10][1000/1563]\tLoss: 0.5751 (0.9100 on avg)\n",
            "Epoch: [9/10][1100/1563]\tLoss: 1.1334 (0.9080 on avg)\n",
            "Epoch: [9/10][1200/1563]\tLoss: 0.8218 (0.9058 on avg)\n",
            "Epoch: [9/10][1300/1563]\tLoss: 1.0262 (0.9027 on avg)\n",
            "Epoch: [9/10][1400/1563]\tLoss: 0.5051 (0.8999 on avg)\n",
            "Epoch: [9/10][1500/1563]\tLoss: 0.9757 (0.8963 on avg)\n",
            "\n",
            "* Epoch: [9/10]\tTrain loss: 0.895\n",
            "\n",
            "Test (on val set): [9/10][   0/313]\tLoss: 1.0477 (1.0477 on avg)\tTop-1 err: 34.3750 (34.3750 on avg)\tTop-5 err: 3.1250 (3.1250 on avg)\n",
            "Test (on val set): [9/10][ 100/313]\tLoss: 0.6050 (1.1480 on avg)\tTop-1 err: 21.8750 (33.1374 on avg)\tTop-5 err: 0.0000 (2.5062 on avg)\n",
            "Test (on val set): [9/10][ 200/313]\tLoss: 1.7804 (1.1206 on avg)\tTop-1 err: 59.3750 (32.3694 on avg)\tTop-5 err: 3.1250 (2.5808 on avg)\n",
            "Test (on val set): [9/10][ 300/313]\tLoss: 1.5524 (1.1008 on avg)\tTop-1 err: 53.1250 (32.1532 on avg)\tTop-5 err: 3.1250 (2.5540 on avg)\n",
            "\n",
            "* Epoch: [9/10]\tTest loss: 1.105\tTop-1 err: 32.260\tTop-5 err: 2.560\n",
            "\n",
            "Dropout: 0.25, Noise: 0.1529, Epoch: 9, Train Loss: 0.8950019468688964, Val Loss: 1.1051635187149047, Top-1 Error: 32.26, Top-5 Error: 2.56\n",
            "Epoch: [10/10][   0/1563]\tLoss: 0.7908 (0.7908 on avg)\n",
            "Epoch: [10/10][ 100/1563]\tLoss: 0.9369 (0.8428 on avg)\n",
            "Epoch: [10/10][ 200/1563]\tLoss: 0.5807 (0.8411 on avg)\n",
            "Epoch: [10/10][ 300/1563]\tLoss: 0.5432 (0.8312 on avg)\n",
            "Epoch: [10/10][ 400/1563]\tLoss: 1.1906 (0.8269 on avg)\n",
            "Epoch: [10/10][ 500/1563]\tLoss: 0.9078 (0.8258 on avg)\n",
            "Epoch: [10/10][ 600/1563]\tLoss: 0.7637 (0.8253 on avg)\n",
            "Epoch: [10/10][ 700/1563]\tLoss: 0.9540 (0.8229 on avg)\n",
            "Epoch: [10/10][ 800/1563]\tLoss: 0.7741 (0.8183 on avg)\n",
            "Epoch: [10/10][ 900/1563]\tLoss: 0.9654 (0.8145 on avg)\n",
            "Epoch: [10/10][1000/1563]\tLoss: 0.9826 (0.8118 on avg)\n",
            "Epoch: [10/10][1100/1563]\tLoss: 0.8717 (0.8082 on avg)\n",
            "Epoch: [10/10][1200/1563]\tLoss: 0.5304 (0.8027 on avg)\n",
            "Epoch: [10/10][1300/1563]\tLoss: 0.5941 (0.8033 on avg)\n",
            "Epoch: [10/10][1400/1563]\tLoss: 0.6357 (0.8016 on avg)\n",
            "Epoch: [10/10][1500/1563]\tLoss: 0.6065 (0.8012 on avg)\n",
            "\n",
            "* Epoch: [10/10]\tTrain loss: 0.799\n",
            "\n",
            "Test (on val set): [10/10][   0/313]\tLoss: 1.2256 (1.2256 on avg)\tTop-1 err: 46.8750 (46.8750 on avg)\tTop-5 err: 3.1250 (3.1250 on avg)\n",
            "Test (on val set): [10/10][ 100/313]\tLoss: 1.1041 (0.9567 on avg)\tTop-1 err: 18.7500 (29.6101 on avg)\tTop-5 err: 0.0000 (1.8564 on avg)\n",
            "Test (on val set): [10/10][ 200/313]\tLoss: 1.1856 (0.9861 on avg)\tTop-1 err: 34.3750 (30.2394 on avg)\tTop-5 err: 3.1250 (2.1611 on avg)\n",
            "Test (on val set): [10/10][ 300/313]\tLoss: 1.1621 (0.9906 on avg)\tTop-1 err: 34.3750 (29.9315 on avg)\tTop-5 err: 3.1250 (2.2010 on avg)\n",
            "\n",
            "* Epoch: [10/10]\tTest loss: 0.991\tTop-1 err: 29.850\tTop-5 err: 2.190\n",
            "\n",
            "Dropout: 0.25, Noise: 0.1529, Epoch: 10, Train Loss: 0.7994142449569702, Val Loss: 0.9906194173812867, Top-1 Error: 29.85, Top-5 Error: 2.19\n"
          ]
        }
      ],
      "source": [
        "# Possible values for dropout probability and noise standard deviation\n",
        "dropout_probs = [0.25]\n",
        "noise_stds = [0.1529]\n",
        "\n",
        "# Function to run an experiment\n",
        "def run_experiment(dropout_p, noise_std, epochs=10):\n",
        "    # Initialize the model with specified dropout and noise parameters\n",
        "    model = CIFAR10Net(dropout_p=dropout_p, noise_std=noise_std)\n",
        "    model.to(device)\n",
        "\n",
        "    # Criterion and optimizer\n",
        "    criterion, optimizer = get_crit_and_opt(model)\n",
        "\n",
        "    # Training and validation\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train(train_loader, model, criterion, optimizer, epoch, epochs)\n",
        "        err1, err5, val_loss = validate(val_loader, model, criterion, epoch, epochs)\n",
        "\n",
        "        # Logging the results\n",
        "        print(f\"Dropout: {dropout_p}, Noise: {noise_std}, Epoch: {epoch+1}, Train Loss: {train_loss}, Val Loss: {val_loss}, Top-1 Error: {err1}, Top-5 Error: {err5}\")\n",
        "\n",
        "# Running all experiments\n",
        "for dropout_p in dropout_probs:\n",
        "    for noise_std in noise_stds:\n",
        "        run_experiment(dropout_p, noise_std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTUR-Z5424KU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "def generate_classification_report(model, val_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)  # Move tensors to the appropriate device\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)  # Forward pass to get the model's predictions\n",
        "            _, predicted = torch.max(outputs, 1)  # Convert outputs to predicted class\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())  # Append true labels to list\n",
        "            y_pred.extend(predicted.cpu().numpy())  # Append predicted labels to list\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Assuming you have 10 classes for CIFAR10, and their names as follows:\n",
        "    target_names = ['airplane', 'bird', 'vegetable', 'dog', 'cat', 'car', 'fruit', 'train', 'rabbit', 'baby']\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(y_true, y_pred, target_names=target_names)\n",
        "    return report\n",
        "\n",
        "# Example usage within an experiment function\n",
        "def run_experiment(dropout_p, noise_std, epochs=10):\n",
        "    model = CIFAR10Net(dropout_p=dropout_p, noise_std=noise_std)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion, optimizer = get_crit_and_opt(model)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train(train_loader, model, criterion, optimizer, epoch, epochs)\n",
        "        err1, err5, val_loss = validate(val_loader, model, criterion, epoch, epochs)\n",
        "\n",
        "    # Generate and print classification report at the end of training\n",
        "    report = generate_classification_report(model, val_loader, device)\n",
        "    print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZHM-Ocg4gfE",
        "outputId": "384953db-d1f1-4b23-f888-0f8bc0e2b9a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.39      0.01      0.02      1000\n",
            "        bird       0.00      0.00      0.00      1000\n",
            "   vegetable       0.00      0.00      0.00      1000\n",
            "         dog       0.00      0.00      0.00      1000\n",
            "         cat       0.00      0.00      0.00      1000\n",
            "         car       0.00      0.00      0.00      1000\n",
            "       fruit       0.00      0.00      0.00      1000\n",
            "       train       0.00      0.00      0.00      1000\n",
            "      rabbit       0.10      1.00      0.18      1000\n",
            "        baby       0.00      0.00      0.00      1000\n",
            "\n",
            "    accuracy                           0.10     10000\n",
            "   macro avg       0.05      0.10      0.02     10000\n",
            "weighted avg       0.05      0.10      0.02     10000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Create a classification report for one model\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation for efficiency\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        # Move tensors to the appropriate device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass to get the model's predictions\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Convert outputs to predicted class by taking the index with the maximum score in each output row\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Append true and predicted labels to lists\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to arrays for compatibility with classification_report\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "#___________________________________________________________________________________________________________________________\n",
        "\n",
        "# Assuming you have 10 classes for CIFAR10, and their names as follows:\n",
        "target_names = ['airplane', 'bird', 'vegetable', 'dog', 'cat',\n",
        "                'car', 'fruit', 'train', 'rabbit', 'baby']\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_true, y_pred, target_names=target_names)\n",
        "\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keRjneACjtUs"
      },
      "source": [
        "Potential Enhancements:\n",
        "Dynamic Learning Rate Adjustment:\n",
        "\n",
        "Implement learning rate schedulers to adjust the learning rate based on training epochs or loss plateaus. This can lead to better training outcomes by adapting the learning rate during training.\n",
        "Augmentation and Regularization:\n",
        "\n",
        "Consider increasing dataset diversity through more advanced data augmentation techniques, which can be particularly effective for preventing overfitting in image recognition tasks.\n",
        "Additional regularization techniques, besides dropout (like L2 regularization), could also be beneficial.\n",
        "Error Handling and User Feedback:\n",
        "\n",
        "Include error handling for file operations (e.g., loading a checkpoint that does not exist) and provide clear messages to the user about what the script is doing or if something goes wrong.\n",
        "Testing and Validation:\n",
        "\n",
        "Implement separate test dataset evaluations if possible, to ensure the model's performance is validated on completely unseen data, which provides a more unbiased evaluation of the model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}